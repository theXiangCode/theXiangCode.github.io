<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Spark核心编程 | The Xiang Blog</title><meta name="keywords" content="RDD,行动算子,转换算子,map,groupBy,shuffle,RDD的分区策略,RDD的创建,flatMap,glom,aggregateByKey,reduceByKey,combineByKey,foldByKey,血缘关系,presist,cache,checkpoint,持久化,依赖关系,分区器,序列化"><meta name="author" content="Xiang Liu"><meta name="copyright" content="Xiang Liu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="介绍了Spark的RDD等核心编程">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark核心编程">
<meta property="og:url" content="http://example.com/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/index.html">
<meta property="og:site_name" content="The Xiang Blog">
<meta property="og:description" content="介绍了Spark的RDD等核心编程">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/image/bg.jpg">
<meta property="article:published_time" content="2022-09-06T11:27:39.924Z">
<meta property="article:modified_time" content="2022-09-08T15:23:27.630Z">
<meta property="article:author" content="Xiang Liu">
<meta property="article:tag" content="RDD">
<meta property="article:tag" content="行动算子">
<meta property="article:tag" content="转换算子">
<meta property="article:tag" content="map">
<meta property="article:tag" content="groupBy">
<meta property="article:tag" content="shuffle">
<meta property="article:tag" content="RDD的分区策略">
<meta property="article:tag" content="RDD的创建">
<meta property="article:tag" content="flatMap">
<meta property="article:tag" content="glom">
<meta property="article:tag" content="aggregateByKey">
<meta property="article:tag" content="reduceByKey">
<meta property="article:tag" content="combineByKey">
<meta property="article:tag" content="foldByKey">
<meta property="article:tag" content="血缘关系">
<meta property="article:tag" content="presist">
<meta property="article:tag" content="cache">
<meta property="article:tag" content="checkpoint">
<meta property="article:tag" content="持久化">
<meta property="article:tag" content="依赖关系">
<meta property="article:tag" content="分区器">
<meta property="article:tag" content="序列化">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/image/bg.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark核心编程',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2022-09-08 23:23:27'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/./image/title.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">28</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">191</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 总览</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/../image/bg.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">The Xiang Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 总览</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Spark核心编程</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-09-06T11:27:39.924Z" title="发表于 2022-09-06 19:27:39">2022-09-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-09-08T15:23:27.630Z" title="更新于 2022-09-08 23:23:27">2022-09-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Spark/">Spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">11.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>46分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark核心编程"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id=""><a href="#" class="headerlink" title=""></a></h2><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Spark计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是：</p>
<ul>
<li><p>RDD : 弹性分布式数据集</p>
</li>
<li><p>累加器：分布式共享只写变量</p>
</li>
<li><p>广播变量：分布式共享只读变量</p>
</li>
</ul>
<p>​	接下来我们一起看看这三大数据结构是如何在数据处理中使用的。</p>
<h2 id="1-RDD"><a href="#1-RDD" class="headerlink" title="1.RDD"></a>1.RDD</h2><h3 id="1-1-什么是RDD？"><a href="#1-1-什么是RDD？" class="headerlink" title="1.1 什么是RDD？"></a>1.1 什么是RDD？</h3><p>​	RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。</p>
<p>Ø  弹性</p>
<ul>
<li><p>存储的弹性：内存与磁盘的自动切换；</p>
</li>
<li><p>容错的弹性：数据丢失可以自动恢复；</p>
</li>
<li><p>计算的弹性：计算出错重试机制；</p>
</li>
<li><p>分片的弹性：可根据需要重新分片。</p>
</li>
</ul>
<p>Ø  分布式：数据存储在大数据集群不同节点上</p>
<p>Ø  数据集：RDD封装了计算逻辑，并不保存数据</p>
<p>Ø  数据抽象：RDD是一个抽象类，需要子类具体实现</p>
<p>Ø  不可变：RDD封装了计算逻辑，是不可以改变的，想要改变，只能产生新的RDD，在新的RDD里面封装计算逻辑</p>
<p>Ø  可分区、并行计算</p>
<h3 id="1-2-核心属性"><a href="#1-2-核心属性" class="headerlink" title="1.2 核心属性"></a>1.2 核心属性</h3><p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662465977252.png" alt="1662465977252"></p>
<p><strong>分区列表</strong></p>
<p>​	RDD数据结构中存在分区列表，用于执行任务时并行计算，是实现分布式计算的重要属性</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662466046354.png" alt="1662466046354"></p>
<p><strong>分区计算函数</strong></p>
<p>​	Spark在计算时，是使用分区函数对每一个分区进行计算</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662466110963.png" alt="1662466110963"></p>
<p><strong>RDD之间的依赖关系</strong></p>
<p>​	RDD是计算模型的封装，当需求中需要将多个计算模型进行组合时，就需要将多个RDD建立依赖关系</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662466141339.png" alt="1662466141339"></p>
<p><strong>分区器（可选）</strong></p>
<p>​	当数据为KV类型数据时，可以通过设定分区器自定义数据的分区</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662466172151.png" alt="1662466172151"></p>
<p><strong>首选位置（可选）</strong></p>
<p>​	计算数据时，可以根据计算节点的状态选择不同的节点位置进行计算</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662466199888.png" alt="1662466199888"></p>
<p>这个首选位置的含义：</p>
<p>当一个task想要分配给某一个executor执行的时候，由于在大数据环境下，我们将会尽可能的把task分配给数据所在的executor之中，因此会给所有可用的executor排序，选最接近数据的那个executor。</p>
<p>在大数据中，移动数据不如移动计算</p>
<h3 id="1-3-执行原理"><a href="#1-3-执行原理" class="headerlink" title="1.3 执行原理"></a>1.3 执行原理</h3><p>​	从计算的角度来讲，数据处理过程中需要计算资源（内存 &amp; CPU）和计算模型（逻辑）。执行时，需要将计算资源和计算模型进行协调和整合。</p>
<p>​	Spark框架在执行时，先申请资源，然后将应用程序的数据处理逻辑分解成一个一个的计算任务。然后将任务发到已经分配资源的计算节点上, 按照指定的计算模型进行数据计算。最后得到计算结果。</p>
<p>​	RDD是Spark框架中用于数据处理的核心模型，接下来我们看看，在Yarn环境中，RDD的工作原理:</p>
<ol>
<li>启动Yarn集群环境</li>
</ol>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662466788800.png" alt="1662466788800"></p>
<ol start="2">
<li>Spark通过申请资源创建调度节点和计算节点</li>
</ol>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662466804123.png" alt="1662466804123"></p>
<ol start="3">
<li>Spark框架根据需求将计算逻辑根据分区划分成不同的任务</li>
</ol>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662466830420.png" alt="1662466830420"></p>
<ol start="4">
<li>调度节点将任务根据计算节点状态发送到对应的计算节点进行计算</li>
</ol>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662466858344.png" alt="1662466858344"></p>
<p>​	从以上流程可以看出RDD在整个流程中主要用于将逻辑进行封装，并生成Task发送给Executor节点执行计算，接下来我们就一起看看Spark框架中RDD是具体是如何进行数据处理的。</p>
<h3 id="1-4-基础编程"><a href="#1-4-基础编程" class="headerlink" title="1.4 基础编程"></a>1.4 基础编程</h3><h4 id="1-4-1-RDD的四种创建方式"><a href="#1-4-1-RDD的四种创建方式" class="headerlink" title="1.4.1 RDD的四种创建方式"></a>1.4.1 RDD的四种创建方式</h4><p>​	在Spark中创建RDD的创建方式可以分为四种：</p>
<p><strong>从集合（内存）中创建RDD</strong></p>
<p>​	从集合中创建RDD，<a target="_blank" rel="noopener" href="https://www.iteblog.com/archives/tag/spark/">Spark</a>主要提供了两个方法：parallelize和makeRDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;spark&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"><span class="keyword">val</span> rdd1 = sparkContext.parallelize(</span><br><span class="line">    <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sparkContext.makeRDD(</span><br><span class="line">    <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">)</span><br><span class="line">rdd1.collect().foreach(println)</span><br><span class="line">rdd2.collect().foreach(println)</span><br><span class="line">sparkContext.stop()</span><br></pre></td></tr></table></figure>

<p>​	从底层代码实现来讲，makeRDD方法其实就是parallelize方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">    numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  parallelize(seq, numSlices)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>从文件中创建RDD</strong></p>
<p>​	由外部存储系统的数据集创建RDD包括：本地的文件系统，所有Hadoop支持的数据集，比如HDFS、HBase等。</p>
<p>​	input路径支持本地路径（绝对、相对）、文件夹、HDFS等等</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;spark&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"><span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;input&quot;</span>)</span><br><span class="line">fileRDD.collect().foreach(println)</span><br><span class="line">sparkContext.stop()</span><br></pre></td></tr></table></figure>

<p><strong>从其它RDD中创建</strong></p>
<p>主要是通过一个RDD运算完后，再产生新的RDD</p>
<p><strong>直接创建RDD（new）</strong></p>
<p>使用new的方式直接构造RDD，一般由Spark框架自身使用</p>
<h4 id="1-4-2-RDD的分区和并行度"><a href="#1-4-2-RDD的分区和并行度" class="headerlink" title="1.4.2 RDD的分区和并行度"></a>1.4.2 RDD的分区和并行度</h4><p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662468520960.png" alt="1662468520960"></p>
<p>​	分区可以提高并行度，达到复杂均衡的效果。</p>
<p>​	默认情况下，Spark可以将一个作业切分多个任务后，发送给Executor节点并行计算，而能够并行计算的任务数量我们称之为并行度。这个数量可以在构建RDD时指定。记住，这里的并行执行的任务数量，并不是指的切分任务的数量，不要混淆了。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;spark&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"><span class="keyword">val</span> dataRDD: <span class="type">RDD</span>[<span class="type">Int</span>] =</span><br><span class="line">    sparkContext.makeRDD(</span><br><span class="line">        <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>),</span><br><span class="line">        <span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] =</span><br><span class="line">    sparkContext.textFile(</span><br><span class="line">        <span class="string">&quot;input&quot;</span>,</span><br><span class="line">        <span class="number">2</span>)</span><br><span class="line">fileRDD.collect().foreach(println)</span><br><span class="line">sparkContext.stop()</span><br></pre></td></tr></table></figure>

<p>​	makeRDD 可以指定创建的RDD的分区数目，</p>
<hr>
<p><strong>利用集合在内存中创建RDD的分区策略</strong></p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662469402062.png" alt="1662469402062"></p>
<p>通过源码我们可以追溯到上述代码，其中length就是集合的元素个数，numSlices代表的切片个数</p>
<p>那么对于第一个分区的范围就是start &#x3D; 0 end &#x3D; 1*len &#x2F; numSlices，</p>
<p>通过上述代码可以计算出所有分区的数字范围，从而得到每一个分区内的数据</p>
<hr>
<p><strong>利用文件创建RDD的分区策略</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">public <span class="type">InputSplit</span>[] getSplits(<span class="type">JobConf</span> job, int numSplits)</span><br><span class="line">    <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line"></span><br><span class="line">    long totalSize = <span class="number">0</span>;                           <span class="comment">// compute total size</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">FileStatus</span> file: files) &#123;                <span class="comment">// check we have valid files</span></span><br><span class="line">      <span class="keyword">if</span> (file.isDirectory()) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">&quot;Not a file: &quot;</span>+ file.getPath());</span><br><span class="line">      &#125;</span><br><span class="line">      totalSize += file.getLen();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    long goalSize = totalSize / (numSplits == <span class="number">0</span> ? <span class="number">1</span> : numSplits);</span><br><span class="line">    long minSize = <span class="type">Math</span>.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.</span><br><span class="line">      <span class="type">FileInputFormat</span>.<span class="type">SPLIT_MINSIZE</span>, <span class="number">1</span>), minSplitSize);</span><br><span class="line">      </span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">FileStatus</span> file: files) &#123;</span><br><span class="line">    </span><br><span class="line">        ...</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (isSplitable(fs, path)) &#123;</span><br><span class="line">          long blockSize = file.getBlockSize();</span><br><span class="line">          long splitSize = computeSplitSize(goalSize, minSize, blockSize);</span><br><span class="line"></span><br><span class="line">          ...</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">protected</span> long computeSplitSize(long goalSize, long minSize,</span><br><span class="line">                                       long blockSize) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="type">Math</span>.max(minSize, <span class="type">Math</span>.min(goalSize, blockSize));</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>利用文件来计算RDD的分区比较麻烦。主要分为两个步骤，第一步是计算真正的分区数目，第二步是计算每一个分区到底该存放那些数据</p>
<p>首先对于第一步计算真正的分区数目，你在创建RDD的时候会输入一个分区数目，这个分区数目就是你预计的分区数目，但并不是真正的分区数目，如果你没有指定分区数目，那么将有内部来计算一个。其实就是计算defaultParallelism和2的最小值作为预计分区数目，而defaultParallelism，这个值其实和上面的集合常见的RDD中计算规则一样，是根据当前环境来计算得出的，默认下是使用的当前环境的虚拟核数目。</p>
<p>得到预计的分区数目之后，我们计算得出总共文件的大小，然后计算预计每一个分区的字节数目，就是利用totalsize除以预期分区数目，为了防止预计分区数目过少，而文件过大，因此后续得到每一个分区真实的字节数目还需要做一个比较，就是计算你预期每一个分区的字节数和blocksize的最小值，计算得到每一个分区应该的字节数目之后，我们再用totalsize除以字节数目，就得到了真实的分区数目，如果有余数，我们需要计算余数字节数目除以每一个分区的字节数，是否大于10%，大于就再多一个分区，不大于，就把数据放到最后的那个分区中。这些概念在hadoop中也有涉及。这样就计算得到了真实的分区数目</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662470735371.png" alt="1662470735371"></p>
<p>得到了真实的分区数目之后，每一个分区真实的字节数目还需要真实介绍，由于我们刚才是按照字节数目来划分的，但是由于spark内部读取文件数据的底层使用的Hadoop，Hadoop的读取数据是按照行来读取的，并不是按照字节来读取，因此还需要计算每一个分区真实存放的字节数目</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662471249739.png" alt="1662471249739"></p>
<p>如下图所示，假设按照字节数目，第一个分区应该是0-3，第二个分区是3-6，第三个分区是6-7，但是3这个偏移量是第二行的，因此第一个分区会把第一行以及第二行全部放到这个分区里面，后续计算过程类似，这样就得到了最后真实的每一个分区存放的数据。</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662472110962.png" alt="1662472110962"></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_45425054/article/details/113899916">(72条消息) Spark之RDD理解（分区策略）_黑星bm的博客-CSDN博客</a></p>
<h4 id="1-4-3-RDD转换算子"><a href="#1-4-3-RDD转换算子" class="headerlink" title="1.4.3 RDD转换算子"></a>1.4.3 RDD转换算子</h4><p>​	转换算子，是对于逻辑的封装，把旧的逻辑转换为新的逻辑</p>
<h5 id="1-4-3-1-map"><a href="#1-4-3-1-map" class="headerlink" title="1.4.3.1 map"></a>1.4.3.1 map</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD1: <span class="type">RDD</span>[<span class="type">Int</span>] = dataRDD.map(</span><br><span class="line">    num =&gt; &#123;</span><br><span class="line">        num * <span class="number">2</span></span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> dataRDD2: <span class="type">RDD</span>[<span class="type">String</span>] = dataRDD1.map(</span><br><span class="line">    num =&gt; &#123;</span><br><span class="line">        <span class="string">&quot;&quot;</span> + num</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>



<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662474951904.png" alt="1662474951904"></p>
<p>​	如果有多个RDD转换算子，而且一个分区内有多条数据需要处理，那么需要把前面的数据把所有的RDD转换算子执行完，才会执行下一条数据。</p>
<h5 id="1-4-3-2-mapPartitions"><a href="#1-4-3-2-mapPartitions" class="headerlink" title="1.4.3.2 mapPartitions"></a>1.4.3.2 mapPartitions</h5><p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662475638410.png" alt="1662475638410"></p>
<p>mappartitions方法，该方法相当于对每一个分区的数据执行一次。所以取到的数据是一个List集合<br><strong>数据处理角度</strong></p>
<p>​	Map算子是分区内一个数据一个数据的执行，类似于串行操作。而mapPartitions算子是以分区为单位进行批处理操作。</p>
<p><strong>功能的角度</strong></p>
<p>​	Map算子主要目的将数据源中的数据进行转换和改变。但是不会减少或增多数据。MapPartitions算子需要传递一个迭代器，返回一个迭代器，没有要求的元素的个数保持不变，所以可以增加或减少数据</p>
<p><strong>性能的角度</strong></p>
<p>​	Map算子因为类似于串行操作，所以性能比较低，而是mapPartitions算子类似于批处理，所以性能较高。但是mapPartitions算子会长时间占用内存，那么这样会导致内存可能不够用，出现内存溢出的错误。所以在内存有限的情况下，不推荐使用。使用map操作。</p>
<h5 id="1-4-3-3-mapPartitionsWithIndex"><a href="#1-4-3-3-mapPartitionsWithIndex" class="headerlink" title="1.4.3.3 mapPartitionsWithIndex"></a>1.4.3.3 mapPartitionsWithIndex</h5><p>​	将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据，在处理时同时可以获取当前分区索引。</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662476260503.png" alt="1662476260503"></p>
<p>参数由1个变为2个，多的那个参数代表第几个分区</p>
<h5 id="1-4-3-4-flatMap"><a href="#1-4-3-4-flatMap" class="headerlink" title="1.4.3.4 flatMap"></a>1.4.3.4 flatMap</h5><p>​	将处理的数据进行扁平化后再进行映射处理，所以算子也称之为扁平映射</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662476457554.png" alt="1662476457554"></p>
<p>一个小练习</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662476588362.png" alt="1662476588362"></p>
<p>​	这里的数据类型不同，既有list类型，还有Int类型，因此需要使用到模式匹配，如果是List类型那就直接返回list，如果是其它类型，那么就直接使用List包装一下。</p>
<h5 id="1-4-3-5-glom"><a href="#1-4-3-5-glom" class="headerlink" title="1.4.3.5 glom"></a>1.4.3.5 glom</h5><p>​	将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变</p>
<p>练习：计算所有分区最大值求和（分区内求最大值，分区间最大值求和）</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662477134288.png" alt="1662477134288"></p>
<h5 id="1-4-3-6-groupBy-shuffle"><a href="#1-4-3-6-groupBy-shuffle" class="headerlink" title="1.4.3.6 groupBy(shuffle)"></a>1.4.3.6 groupBy(shuffle)</h5><p>​	将数据根据指定的规则进行分组, 分区默认不变，但是数据会被打乱重新组合，我们将这样的操作称之为shuffle。极限情况下，数据可能被分在同一个分区中</p>
<p>​	<strong>一个组的数据在一个分区中，但是并不是说一个分区中只有一个组</strong></p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662477321533.png" alt="1662477321533"></p>
<p>​	默认情况下，数据处理之后，所在分区不会发生改变</p>
<p>​	Spark要求，一个组的数据必须在一个分区中</p>
<p>一个分区的数据被打乱重新和其它分区的数据组合在一起，这个操作叫做shuffle</p>
<p>shuffle操作不允许在内存中等待，必须要落盘</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662477719598.png" alt="1662477719598"></p>
<p>如上图所示，如果我们RDD中存在很多个分区，后面紧跟了个shuffle操作，那么如果在内存中执行，我们需要等待RDD所有的分区执行结束后才能接着往下走，但是这个过程极其耗费时间，而且十分耗费内存。</p>
<p>因此shuffle不允许在内存中等待，必须要落盘</p>
<p>shuffle会把完整的计算过程分为两个部分，一个阶段用于写数据，一个阶段用于读数据</p>
<p>写数据的阶段没有完成，读数据的操作不能执行</p>
<p>shuffle的操作可以更改分区数目</p>
<h5 id="1-4-3-7-filter"><a href="#1-4-3-7-filter" class="headerlink" title="1.4.3.7 filter"></a>1.4.3.7 filter</h5><p>​	将数据根据指定的规则进行筛选过滤，符合规则的数据保留，不符合规则的数据丢弃。当数据进行筛选过滤后，分区不变，但是分区内的数据可能不均衡，生产环境下，可能会出现数据倾斜。</p>
<p><strong>什么叫数据倾斜？</strong></p>
<p>​	分区之间数据十分不均衡就是数据倾斜</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">    <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span></span><br><span class="line">),<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.filter(_%<span class="number">2</span> == <span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h5 id="1-4-3-8-sample"><a href="#1-4-3-8-sample" class="headerlink" title="1.4.3.8 sample"></a>1.4.3.8 sample</h5><p>​	根据指定的规则从数据集中抽取数据 </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>),<span class="number">1</span>)</span><br><span class="line"><span class="comment">// 抽取数据不放回（伯努利算法）</span></span><br><span class="line"><span class="comment">// 伯努利算法：又叫0、1分布。例如扔硬币，要么正面，要么反面。</span></span><br><span class="line"><span class="comment">// 具体实现：根据种子和随机算法算出一个数和第二个参数设置几率比较，小于第二个参数要，大于不要</span></span><br><span class="line"><span class="comment">// 第一个参数：抽取的数据是否放回，false：不放回</span></span><br><span class="line"><span class="comment">// 第二个参数：抽取的几率，范围在[0,1]之间,0：全不取；1：全取；</span></span><br><span class="line"><span class="comment">// 第三个参数：随机数种子</span></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.sample(<span class="literal">false</span>, <span class="number">0.5</span>)</span><br><span class="line"><span class="comment">// 抽取数据放回（泊松算法）</span></span><br><span class="line"><span class="comment">// 第一个参数：抽取的数据是否放回，true：放回；false：不放回</span></span><br><span class="line"><span class="comment">// 第二个参数：重复数据的几率，范围大于等于0.表示每一个元素被期望抽取到的次数</span></span><br><span class="line"><span class="comment">// 第三个参数：随机数种子</span></span><br><span class="line"><span class="keyword">val</span> dataRDD2 = dataRDD.sample(<span class="literal">true</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>我们可以使用sample取部分数据，来判断这个数据经过后面的数据转换是否会发生数据倾斜的现象</p>
<h5 id="1-4-3-9-distinct-shuffle"><a href="#1-4-3-9-distinct-shuffle" class="headerlink" title="1.4.3.9 distinct (shuffle)"></a>1.4.3.9 distinct (shuffle)</h5><p>​	将数据集中重复的数据去重</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>),<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.distinct()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD2 = dataRDD.distinct(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>​	由于RDD中是存在分区的概念的，那spark是如何实现的去重操作呢？</p>
<p>​	内部会把每一条数据作为key，value填充null，然后使用reduceBykey操作，该操作只会对于相同key值的value进行聚合，由于value值都是null，最终把相同key值的所有值转换为（key,null)，最后在经过一个map操作把key得到就实现了去重操作。</p>
<h5 id="1-4-3-10-coalesce-shuffle"><a href="#1-4-3-10-coalesce-shuffle" class="headerlink" title="1.4.3.10 coalesce(shuffle)"></a>1.4.3.10 coalesce(shuffle)</h5><p>​	根据数据量缩减分区，用于大数据集过滤后，提高小数据集的执行效率</p>
<p>​	当spark程序中，存在过多的小任务的时候，可以通过coalesce方法，收缩合并分区，减少分区的个数，减小任务调度成本</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>),<span class="number">6</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.coalesce(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>​	这种方式在某些情况下，无法解决数据倾斜的问题，所以还可以在缩减分区的同时，进行数据的shuffle操作 </p>
<p>​	由于coalesce方法并不是完全按照几个数据量小的来合并，由于他根本不清楚每一个分区的数量到底是多大，因此它使用的是首选位置来进行合并，就是按照数据所在节点之间的距离来抉择。因此无法解决数据倾斜的问题。</p>
<p>​	该方法还可以传入第二个参数，指定是否进行shuffle操作，如果值为false并不会执行shuffle，shuffle是把一个分区内的数据打乱放到其它分区中，此处只是把多个分区间的数据进行合并，所以并不算shuffle操作。其执行shuffle后数据会稍微均衡一点，但也无法做到十分均衡。后续还会接着介绍。</p>
<p>​	该方法的第一个参数是分区数目，该数目还可以大于之前的分区数目，并不是一定要缩减分区。</p>
<p>但是当大于之前的分区数目的时候，如果没有打开shuffle操作，将不能把一个分区的数据打乱放到其它分区内，所以并不会执行，扩大分区无效。只有开启shuffle才会增大分区数目。</p>
<h5 id="1-4-3-11-repartition-shuffle"><a href="#1-4-3-11-repartition-shuffle" class="headerlink" title="1.4.3.11 repartition (shuffle)"></a>1.4.3.11 repartition (shuffle)</h5><p>​	该操作内部其实执行的是coalesce操作，参数shuffle的默认值为true。无论是将分区数多的RDD转换为分区数少的RDD，还是将分区数少的RDD转换为分区数多的RDD，repartition操作都可以完成，因为无论如何都会经shuffle过程。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">    <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span></span><br><span class="line">),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.repartition(<span class="number">4</span>)</span><br></pre></td></tr></table></figure>

<p>因此之后我们扩大分区可以使用repartition函数，缩减分区使用coalesce函数。</p>
<h5 id="1-4-3-12-sortBy-shuffle"><a href="#1-4-3-12-sortBy-shuffle" class="headerlink" title="1.4.3.12 sortBy (shuffle)"></a>1.4.3.12 sortBy (shuffle)</h5><p>​	该操作用于排序数据。在排序之前，可以将数据通过f函数进行处理，之后按照f函数处理的结果进行排序，默认为升序排列。排序后新产生的RDD的分区数与原RDD的分区数一致。中间存在shuffle的过程</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">    <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span></span><br><span class="line">),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.sortBy(num=&gt;num, <span class="literal">false</span>, <span class="number">4</span>) <span class="comment">//false参数代表降序排列</span></span><br></pre></td></tr></table></figure>

<p>但是我目前产生一个疑问，RDD是存在分区概念的，那么它是如何实现的所有分区内的数据排好顺序呢？</p>
<p>实验一波</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662547994160.png" alt="1662547994160"></p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662548008359.png" alt="1662548008359"></p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662548014650.png" alt="1662548014650"></p>
<p>sortBy之后</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662548075504.png" alt="1662548075504"></p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662548086800.png" alt="1662548086800"></p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662548095986.png" alt="1662548095986"></p>
<p>因此我们可以看出，他是肯定会执行一个shuffle操作，然后按照分区号码，把更小的数字放到分区号小的位置中去，并且分区间也是有序的。</p>
<h5 id="1-4-3-13-intersection"><a href="#1-4-3-13-intersection" class="headerlink" title="1.4.3.13 intersection"></a>1.4.3.13 intersection</h5><p>​	对源RDD和参数RDD求交集后返回一个新的RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD = dataRDD1.intersection(dataRDD2)</span><br></pre></td></tr></table></figure>



<h5 id="1-4-3-14-union"><a href="#1-4-3-14-union" class="headerlink" title="1.4.3.14 union"></a>1.4.3.14 union</h5><p>​	以一个RDD元素为主，去除两个RDD中重复元素，将其他元素保留下来。求差集</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD = dataRDD1.union(dataRDD2)</span><br></pre></td></tr></table></figure>



<h5 id="1-4-3-15-subtract"><a href="#1-4-3-15-subtract" class="headerlink" title="1.4.3.15 subtract"></a>1.4.3.15 subtract</h5><p>​	以一个RDD元素为主，去除两个RDD中重复元素，将其他元素保留下来。求差集</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD = dataRDD1.subtract(dataRDD2)</span><br></pre></td></tr></table></figure>

<h5 id="1-4-3-16-zip"><a href="#1-4-3-16-zip" class="headerlink" title="1.4.3.16 zip"></a>1.4.3.16 zip</h5><p>​	将两个RDD中的元素，以键值对的形式进行合并。其中，键值对中的Key为第1个RDD中的元素，Value为第2个RDD中的相同位置的元素。</p>
<p>​	但是必须要满足两个RDD的元素类型相同，以及RDD每一个分区之间的数目要相同</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662548984343.png" alt="1662548984343"></p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662549021025.png" alt="1662549021025"></p>
<h5 id="1-4-3-17-partitionBy"><a href="#1-4-3-17-partitionBy" class="headerlink" title="1.4.3.17 partitionBy"></a>1.4.3.17 partitionBy</h5><p>​	将数据按照指定Partitioner重新进行分区。Spark默认的分区器是HashPartitioner</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] =</span><br><span class="line">    sc.makeRDD(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">&quot;aaa&quot;</span>),(<span class="number">2</span>,<span class="string">&quot;bbb&quot;</span>),(<span class="number">3</span>,<span class="string">&quot;ccc&quot;</span>)),<span class="number">3</span>)</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">HashPartitioner</span></span><br><span class="line"><span class="keyword">val</span> rdd2: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] =</span><br><span class="line">    rdd.partitionBy(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br></pre></td></tr></table></figure>

<p>partitionBy算子是根据指定的规则对每一条数据进行重新分区</p>
<p>repartition：强调分区数量的变换，对于数据并不关心</p>
<p>partitionBy：强调把数据放到哪一个分区</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662549392283.png" alt="1662549392283"></p>
<p>使用的是默认Spark中默认的shuffle分区器，HashPartitioner分区器</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662549585805.png" alt="1662549585805"></p>
<p>如果key是null，放到0号分区，如果有值就是按照key的hashcode和分区数目取余数得到数据所在分区数</p>
<h5 id="1-4-3-18-reduceByKey"><a href="#1-4-3-18-reduceByKey" class="headerlink" title="1.4.3.18  reduceByKey"></a>1.4.3.18  reduceByKey</h5><p>​	可以将数据按照相同的Key对Value进行聚合</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = dataRDD1.reduceByKey(_+_)</span><br><span class="line"><span class="keyword">val</span> dataRDD3 = dataRDD1.reduceByKey(_+_, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>对于相同的key，把value进行聚合</p>
<h5 id="1-4-3-19-groupByKey"><a href="#1-4-3-19-groupByKey" class="headerlink" title="1.4.3.19 groupByKey"></a>1.4.3.19 groupByKey</h5><p>​	将数据源的数据根据key对value进行分组</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 =</span><br><span class="line">    sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = dataRDD1.groupByKey()</span><br><span class="line"><span class="keyword">val</span> dataRDD3 = dataRDD1.groupByKey(<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD4 = dataRDD1.groupByKey(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br></pre></td></tr></table></figure>

<p>​	该算子把相同key数据的value放在一个组中</p>
<p>groupBy和groupByKey的区别</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662550361082.png" alt="1662550361082"></p>
<p>reduceByKey和groupByKey区别</p>
<p><strong>从shuffle的角度</strong>：reduceByKey和groupByKey都存在shuffle的操作，<strong>但是reduceByKey可以在shuffle前对分区内相同key的数据进行预聚合（combine）功能</strong>，这样会减少落盘的数据量，而groupByKey只是进行分组，不存在数据量减少的问题，reduceByKey性能比较高。</p>
<p><strong>从功能的角度</strong>：reduceByKey其实包含分组和聚合的功能。groupByKey只能分组，不能聚合，所以在分组聚合的场合下，推荐使用reduceByKey，如果仅仅是分组而不需要聚合。那么还是只能使用groupByKey</p>
<h5 id="1-4-3-20-aggregateByKey"><a href="#1-4-3-20-aggregateByKey" class="headerlink" title="1.4.3.20 aggregateByKey"></a>1.4.3.20 aggregateByKey</h5><p>​	将数据根据不同的规则进行分区内计算和分区间计算</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662561299897.png" alt="1662561299897"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 =</span><br><span class="line">    sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 =</span><br><span class="line">    dataRDD1.aggregateByKey(<span class="number">0</span>)(_+_,_+_)</span><br></pre></td></tr></table></figure>



<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TODO : 取出每个分区内相同key的最大值然后分区间相加</span></span><br><span class="line"><span class="comment">// aggregateByKey算子是函数柯里化，存在两个参数列表</span></span><br><span class="line"><span class="comment">// 1. 第一个参数列表中的参数表示初始值</span></span><br><span class="line"><span class="comment">// 2. 第二个参数列表中含有两个参数</span></span><br><span class="line"><span class="comment">//    2.1 第一个参数表示分区内的计算规则</span></span><br><span class="line"><span class="comment">//    2.2 第二个参数表示分区间的计算规则</span></span><br><span class="line"><span class="keyword">val</span> rdd =</span><br><span class="line">    sc.makeRDD(<span class="type">List</span>(</span><br><span class="line">        (<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;a&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>),</span><br><span class="line">        (<span class="string">&quot;b&quot;</span>,<span class="number">4</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">5</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">6</span>)</span><br><span class="line">    ),<span class="number">2</span>)</span><br><span class="line"><span class="comment">// 0:(&quot;a&quot;,1),(&quot;a&quot;,2),(&quot;c&quot;,3) =&gt; (a,10)(c,10)</span></span><br><span class="line"><span class="comment">//                                         =&gt; (a,10)(b,10)(c,20)</span></span><br><span class="line"><span class="comment">// 1:(&quot;b&quot;,4),(&quot;c&quot;,5),(&quot;c&quot;,6) =&gt; (b,10)(c,10)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> resultRDD =</span><br><span class="line">    rdd.aggregateByKey(<span class="number">10</span>)(</span><br><span class="line">        (x, y) =&gt; math.max(x,y),</span><br><span class="line">        (x, y) =&gt; x + y</span><br><span class="line">    )</span><br><span class="line">resultRDD.collect().foreach(println)</span><br></pre></td></tr></table></figure>

<p>​	第一个参数是计算初始值，主要是可能某一个分区内的key只有一个，从而无法执行聚合计算，因此需要一个初始值进行聚合计算过程</p>
<p>​	该函数主要是可以实现分区间一个计算规则、分区内一个计算规则，当遇到这种需求的时候，可以使用这个算子，而刚才的reduceByKey是分区内和分区间都一个计算规则。</p>
<p>​	分区之间存在shuffle.</p>
<h5 id="1-4-3-21-foldByKey"><a href="#1-4-3-21-foldByKey" class="headerlink" title="1.4.3.21 foldByKey"></a>1.4.3.21 foldByKey</h5><p>​	当分区内计算规则和分区间计算规则相同时，aggregateByKey就可以简化为foldByKey</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = dataRDD1.foldByKey(<span class="number">0</span>)(_+_)</span><br></pre></td></tr></table></figure>

<h5 id="1-4-3-22-combineByKey"><a href="#1-4-3-22-combineByKey" class="headerlink" title="1.4.3.22 combineByKey"></a>1.4.3.22 combineByKey</h5><p>​	最通用的对key-value型rdd进行聚集操作的聚集函数（aggregation function）。类似于aggregate()，combineByKey()允许用户返回值的类型与输入不一致。</p>
<p>​	小练习：将数据List((“a”, 88),(“b”, 95), (“a”, 91), (“b”, 93), (“a”,95), (“b”, 98))求每个key的平均值</p>
<p>conbineByKey有三个参数</p>
<p>第一个参数表示 当属不符合我们的规则的时候，用于进行转换的操作</p>
<p>第二个参数表示 分区内的计算规则</p>
<p>第三个参数表示 分区间计算规则</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">88</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">95</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">91</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">93</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">95</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">98</span>))</span><br><span class="line"><span class="keyword">val</span> input: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(list, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> combineRdd: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>))] = input.combineByKey(</span><br><span class="line">    (_, <span class="number">1</span>),</span><br><span class="line">    (acc: (<span class="type">Int</span>, <span class="type">Int</span>), v) =&gt; (acc._1 + v, acc._2 + <span class="number">1</span>),</span><br><span class="line">    (acc1: (<span class="type">Int</span>, <span class="type">Int</span>), acc2: (<span class="type">Int</span>, <span class="type">Int</span>)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>​	第一个参数好像只会对于每一个分区内的每一个key的第一个数据进行数据的转化，而对于该key的其它的值会进行第二个参数的规则来计算。</p>
<p>reduceByKey: 相同key的第一个数据不进行任何计算，分区内和分区间计算规则相同</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662563755802.png" alt="1662563755802"></p>
<p>foldByKey: 相同key的第一个数据和初始值进行分区内计算，分区内和分区间计算规则相同</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662563847368.png" alt="1662563847368"></p>
<p>aggregateByKey：相同key的第一个数据和初始值进行分区内计算，分区内和分区间计算规则可以不相同</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662563800127.png" alt="1662563800127"></p>
<p>combineByKey:当计算时，发现数据结构不满足要求时，可以让第一个数据转换结构。分区内和分区间计算规则不相同。</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662563880705.png" alt="1662563880705"></p>
<h5 id="1-4-3-23-sortByKey"><a href="#1-4-3-23-sortByKey" class="headerlink" title="1.4.3.23 sortByKey"></a>1.4.3.23 sortByKey</h5><p>​	在一个(K,V)的RDD上调用，K必须实现Ordered接口(特质)，返回一个按照key进行排序的</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> sortRDD1: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = dataRDD1.sortByKey(<span class="literal">true</span>)</span><br><span class="line"><span class="keyword">val</span> sortRDD1: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = dataRDD1.sortByKey(<span class="literal">false</span>)</span><br></pre></td></tr></table></figure>

<h5 id="1-4-3-24-join"><a href="#1-4-3-24-join" class="headerlink" title="1.4.3.24 join"></a>1.4.3.24 join</h5><p>​	在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素连接在一起的(K,(V,W))的RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = sc.makeRDD(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;b&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;c&quot;</span>)))</span><br><span class="line"><span class="keyword">val</span> rdd1: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = sc.makeRDD(<span class="type">Array</span>((<span class="number">1</span>, <span class="number">4</span>), (<span class="number">2</span>, <span class="number">5</span>), (<span class="number">3</span>, <span class="number">6</span>)))</span><br><span class="line">rdd.join(rdd1).collect().foreach(println)</span><br></pre></td></tr></table></figure>

<p>​	但是这个join操作可能会产生笛卡尔乘积，可能左边有1w个a，右边的也有1w个a，这样会产生1亿个数据，因此效率比较低，而且他由于需要把分区间的key做连接，所以需要用到shuffle操作，落盘效率较低</p>
<h5 id="1-4-3-25-leftOuterJoin"><a href="#1-4-3-25-leftOuterJoin" class="headerlink" title="1.4.3.25 leftOuterJoin"></a>1.4.3.25 leftOuterJoin</h5><p>​	类似于SQL语句的左外连接</p>
<p><strong>数据：</strong></p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662564991854.png" alt="1662564991854"></p>
<p><strong>操作：</strong></p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662565024672.png" alt="1662565024672"></p>
<p><strong>结果：</strong></p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662565046824.png" alt="1662565046824"></p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662565195311.png" alt="1662565195311"></p>
<h5 id="1-4-3-26-cogroup"><a href="#1-4-3-26-cogroup" class="headerlink" title="1.4.3.26 cogroup"></a>1.4.3.26 cogroup</h5><p>​	cogroup 相当于connect+group 组内连接</p>
<p>​	在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<V>,Iterable<W>))类型的RDD</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662565251108.png" alt="1662565251108"></p>
<p>​	先把一个分区内的相同key做了一个分组，然后再和另外一个表进行连接，</p>
<h4 id="1-4-4-RDD行动算子"><a href="#1-4-4-RDD行动算子" class="headerlink" title="1.4.4 RDD行动算子"></a>1.4.4 RDD行动算子</h4><p>​	执行逻辑，把封装好的逻辑执行起来。</p>
<p>​	一个行动算子，就会提交一个作业</p>
<h5 id="1-4-4-1-reduce"><a href="#1-4-4-1-reduce" class="headerlink" title="1.4.4.1 reduce"></a>1.4.4.1 reduce</h5><p>​	该函数会对数据进行聚合操作，和scala中的reduce方法不同，spark的reduce算子是分布式的，因此首先会对分区内数据进行聚合，然后在对分区间进行聚合。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 聚合数据</span></span><br><span class="line"><span class="keyword">val</span> reduceResult: <span class="type">Int</span> = rdd.reduce(_+_)</span><br></pre></td></tr></table></figure>

<h5 id="1-4-4-2-collect"><a href="#1-4-4-2-collect" class="headerlink" title="1.4.4.2 collect"></a>1.4.4.2 collect</h5><p>​	把数据从executor中采集数据到driver之中，但是可能driver中的内存不足，此时会产生溢写操作，因此如果数据量比较大，不要使用collect操作</p>
<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662636913213.png" alt="1662636913213"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 收集数据到Driver</span></span><br><span class="line">rdd.collect().foreach(println)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h5 id="1-4-4-3-count"><a href="#1-4-4-3-count" class="headerlink" title="1.4.4.3 count"></a>1.4.4.3 count</h5><p>​	函数作用很简单，就是计算RDD中元素的个数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 返回RDD中元素的个数</span></span><br><span class="line"><span class="keyword">val</span> countResult: <span class="type">Long</span> = rdd.count()</span><br></pre></td></tr></table></figure>

<h5 id="1-4-4-4-first"><a href="#1-4-4-4-first" class="headerlink" title="1.4.4.4 first"></a>1.4.4.4 first</h5><p>​	函数的作用是取出RDD中第一个元素</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 返回RDD中元素的个数</span></span><br><span class="line"><span class="keyword">val</span> firstResult: <span class="type">Int</span> = rdd.first()</span><br><span class="line">println(firstResult</span><br></pre></td></tr></table></figure>



<h5 id="1-4-4-5-take"><a href="#1-4-4-5-take" class="headerlink" title="1.4.4.5 take"></a>1.4.4.5 take</h5><p>​	返回一个由RDD的前n个元素组成的数组</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vval rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 返回RDD中元素的个数</span></span><br><span class="line"><span class="keyword">val</span> takeResult: <span class="type">Array</span>[<span class="type">Int</span>] = rdd.take(<span class="number">2</span>)</span><br><span class="line">println(takeResult.mkString(<span class="string">&quot;,&quot;</span>))</span><br></pre></td></tr></table></figure>



<h5 id="1-4-4-6-takeOrdered"><a href="#1-4-4-6-takeOrdered" class="headerlink" title="1.4.4.6 takeOrdered"></a>1.4.4.6 takeOrdered</h5><pre><code> 返回该**RDD排序后**的前n个元素组成的数组
</code></pre>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 返回RDD中元素的个数</span></span><br><span class="line"><span class="keyword">val</span> result: <span class="type">Array</span>[<span class="type">Int</span>] = rdd.takeOrdered(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>



<h5 id="1-4-4-7-aggregate"><a href="#1-4-4-7-aggregate" class="headerlink" title="1.4.4.7 aggregate"></a>1.4.4.7 aggregate</h5><p>​	分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将该RDD所有元素相加得到结果</span></span><br><span class="line"><span class="comment">//val result: Int = rdd.aggregate(0)(_ + _, _ + _)</span></span><br><span class="line"><span class="keyword">val</span> result: <span class="type">Int</span> = rdd.aggregate(<span class="number">10</span>)(_ + _, _ + _)</span><br></pre></td></tr></table></figure>

<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662637655394.png" alt="1662637655394"></p>
<h5 id="1-4-4-8-fold"><a href="#1-4-4-8-fold" class="headerlink" title="1.4.4.8 fold"></a>1.4.4.8 fold</h5><p>​	折叠操作，aggregate的简化版操作</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> foldResult: <span class="type">Int</span> = rdd.fold(<span class="number">0</span>)(_+_)</span><br></pre></td></tr></table></figure>

<h5 id="1-4-4-9-countByKey"><a href="#1-4-4-9-countByKey" class="headerlink" title="1.4.4.9 countByKey"></a>1.4.4.9 countByKey</h5><p>​	 统计每种key的个数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = sc.makeRDD(<span class="type">List</span>((<span class="number">1</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">1</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">1</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;b&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;c&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;c&quot;</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 统计每种key的个数</span></span><br><span class="line"><span class="keyword">val</span> result: collection.<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Long</span>] = rdd.countByKey()</span><br></pre></td></tr></table></figure>

<h5 id="1-4-4-10-save相关算子"><a href="#1-4-4-10-save相关算子" class="headerlink" title="1.4.4.10 save相关算子"></a>1.4.4.10 save相关算子</h5><p>​	将数据保存到不同格式的文件中</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 保存成Text文件</span></span><br><span class="line">rdd.saveAsTextFile(<span class="string">&quot;output&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 序列化成对象保存到文件</span></span><br><span class="line">rdd.saveAsObjectFile(<span class="string">&quot;output1&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 保存成Sequencefile文件</span></span><br><span class="line">rdd.map((_,<span class="number">1</span>)).saveAsSequenceFile(<span class="string">&quot;output2&quot;</span>)</span><br></pre></td></tr></table></figure>



<h5 id="1-4-4-11-foreach"><a href="#1-4-4-11-foreach" class="headerlink" title="1.4.4.11 foreach"></a>1.4.4.11 foreach</h5><p>​	分布式遍历RDD中的每一个元素，调用指定函数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 收集后打印</span></span><br><span class="line">rdd.map(num=&gt;num).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">println(<span class="string">&quot;****************&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 分布式打印</span></span><br><span class="line">rdd.foreach(println)</span><br></pre></td></tr></table></figure>

<h4 id="1-4-5-RDD序列化"><a href="#1-4-5-RDD序列化" class="headerlink" title="1.4.5 RDD序列化"></a>1.4.5 RDD序列化</h4><p><strong>闭包检查</strong></p>
<p>​	从计算的角度, 算子以外的代码都是在Driver端执行, 算子里面的代码都是在Executor端执行。那么在scala的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就形成了闭包的效果，如果使用的算子外的数据无法序列化，就意味着无法传值给Executor端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。Scala2.12版本后闭包编译方式发生了改变</p>
<p><strong>序列化方法和属性</strong></p>
<p>​	<strong>从计算的角度, 算子以外的代码都是在Driver端执行, 算子里面的代码都是在Executor端执行，看如下代码：</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">serializable02_function</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//1.创建SparkConf并设置App名称</span></span><br><span class="line">        <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;SparkCoreTest&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.创建SparkContext，该对象是提交Spark App的入口</span></span><br><span class="line">        <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.创建一个RDD</span></span><br><span class="line">        <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.makeRDD(<span class="type">Array</span>(<span class="string">&quot;hello world&quot;</span>, <span class="string">&quot;hello spark&quot;</span>, <span class="string">&quot;hive&quot;</span>, <span class="string">&quot;atguigu&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.1创建一个Search对象</span></span><br><span class="line">        <span class="keyword">val</span> search = <span class="keyword">new</span> <span class="type">Search</span>(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.2 函数传递，打印：ERROR Task not serializable</span></span><br><span class="line">        search.getMatch1(rdd).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.3 属性传递，打印：ERROR Task not serializable</span></span><br><span class="line">        search.getMatch2(rdd).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4.关闭连接</span></span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Search</span>(<span class="params">query:<span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isMatch</span></span>(s: <span class="type">String</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">        s.contains(query)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 函数序列化案例</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getMatch1</span> </span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">        <span class="comment">//rdd.filter(this.isMatch)</span></span><br><span class="line">        rdd.filter(isMatch)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 属性序列化案例</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getMatch2</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">        <span class="comment">//rdd.filter(x =&gt; x.contains(this.query))</span></span><br><span class="line">        rdd.filter(x =&gt; x.contains(query))</span><br><span class="line">        <span class="comment">//val q = query</span></span><br><span class="line">        <span class="comment">//rdd.filter(x =&gt; x.contains(q))</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>Kryo序列化框架</strong>	</p>
<p>​	参考地址: <a target="_blank" rel="noopener" href="https://github.com/EsotericSoftware/kryo">https://github.com/EsotericSoftware/kryo</a></p>
<p>​	Java的序列化能够序列化任何的类。但是比较重（字节多），序列化后，对象的提交也比较大。Spark出于性能的考虑，Spark2.0开始支持另外一种Kryo序列化机制。Kryo速度是Serializable的10倍。当RDD在Shuffle数据的时候，简单数据类型、数组和字符串类型已经在Spark内部使用Kryo来序列化。</p>
<p>​	注意：即使使用Kryo序列化，也要继承Serializable接口。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">serializable_Kryo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">                .setAppName(<span class="string">&quot;SerDemo&quot;</span>)</span><br><span class="line">                .setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">                <span class="comment">// 替换默认的序列化机制</span></span><br><span class="line">                .set(<span class="string">&quot;spark.serializer&quot;</span>, <span class="string">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>)</span><br><span class="line">                <span class="comment">// 注册需要使用 kryo 序列化的自定义类</span></span><br><span class="line">                .registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">Searcher</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.makeRDD(<span class="type">Array</span>(<span class="string">&quot;hello world&quot;</span>, <span class="string">&quot;hello atguigu&quot;</span>, <span class="string">&quot;atguigu&quot;</span>, <span class="string">&quot;hahah&quot;</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> searcher = <span class="keyword">new</span> <span class="type">Searcher</span>(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> result: <span class="type">RDD</span>[<span class="type">String</span>] = searcher.getMatchedRDD1(rdd)</span><br><span class="line"></span><br><span class="line">        result.collect.foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Searcher</span>(<span class="params">val query: <span class="type">String</span></span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isMatch</span></span>(s: <span class="type">String</span>) = &#123;</span><br><span class="line">        s.contains(query)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getMatchedRDD1</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]) = &#123;</span><br><span class="line">        rdd.filter(isMatch) </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getMatchedRDD2</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]) = &#123;</span><br><span class="line">        <span class="keyword">val</span> q = query</span><br><span class="line">        rdd.filter(_.contains(q))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="1-4-6-RDD依赖关系"><a href="#1-4-6-RDD依赖关系" class="headerlink" title="1.4.6 RDD依赖关系"></a>1.4.6 RDD依赖关系</h4><p><strong>血缘关系</strong></p>
<p>​	RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;input/1.txt&quot;</span>)</span><br><span class="line">println(fileRDD.toDebugString)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> wordRDD: <span class="type">RDD</span>[<span class="type">String</span>] = fileRDD.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">println(wordRDD.toDebugString)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordRDD.map((_,<span class="number">1</span>))</span><br><span class="line">println(mapRDD.toDebugString)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> resultRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = mapRDD.reduceByKey(_+_)</span><br><span class="line">println(resultRDD.toDebugString)</span><br><span class="line"></span><br><span class="line">resultRDD.collect()</span><br></pre></td></tr></table></figure>

<p><strong>RDD依赖关系</strong></p>
<p>​	这里所谓的依赖关系，其实就是两个相邻RDD之间的关系</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;input/1.txt&quot;</span>)</span><br><span class="line">println(fileRDD.dependencies)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> wordRDD: <span class="type">RDD</span>[<span class="type">String</span>] = fileRDD.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">println(wordRDD.dependencies)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordRDD.map((_,<span class="number">1</span>))</span><br><span class="line">println(mapRDD.dependencies)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> resultRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = mapRDD.reduceByKey(_+_)</span><br><span class="line">println(resultRDD.dependencies)</span><br><span class="line"></span><br><span class="line">resultRDD.collect()</span><br></pre></td></tr></table></figure>



<p><strong>RDD窄依赖</strong></p>
<p>​	窄依赖表示每一个父(上游)RDD的Partition最多被子（下游）RDD的一个Partition使用，窄依赖我们形象的比喻为独生子女。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OneToOneDependency</span>[<span class="type">T</span>](<span class="params">rdd: <span class="type">RDD</span>[<span class="type">T</span>]</span>) <span class="keyword">extends</span> <span class="title">NarrowDependency</span>[<span class="type">T</span>](<span class="params">rdd</span>) </span></span><br></pre></td></tr></table></figure>

<p><strong>RDD宽依赖</strong></p>
<p>​	宽依赖表示同一个父（上游）RDD的Partition被多个子（下游）RDD的Partition依赖，会引起Shuffle，总结：宽依赖我们形象的比喻为多生</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShuffleDependency</span>[<span class="type">K</span>: <span class="type">ClassTag</span>, <span class="type">V</span>: <span class="type">ClassTag</span>, <span class="type">C</span>: <span class="type">ClassTag</span>](<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    @transient private val _rdd: <span class="type">RDD</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]],</span></span></span><br><span class="line"><span class="params"><span class="class">    val partitioner: <span class="type">Partitioner</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    val serializer: <span class="type">Serializer</span> = <span class="type">SparkEnv</span>.get.serializer,</span></span></span><br><span class="line"><span class="params"><span class="class">    val keyOrdering: <span class="type">Option</span>[<span class="type">Ordering</span>[<span class="type">K</span>]] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    val aggregator: <span class="type">Option</span>[<span class="type">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    val mapSideCombine: <span class="type">Boolean</span> = false</span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">Dependency</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]] </span><br></pre></td></tr></table></figure>

<p><strong>RDD阶段划分</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="comment">// New stage creation may throw an exception if, for example, jobs are run on a</span></span><br><span class="line">  <span class="comment">// HadoopRDD whose underlying HDFS files have been deleted.</span></span><br><span class="line">  finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)</span><br><span class="line">&#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">    logWarning(<span class="string">&quot;Creating new stage failed due to exception - job: &quot;</span> + jobId, e)</span><br><span class="line">    listener.jobFailed(e)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">……</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createResultStage</span></span>(</span><br><span class="line">  rdd: <span class="type">RDD</span>[_],</span><br><span class="line">  func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,</span><br><span class="line">  partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">  jobId: <span class="type">Int</span>,</span><br><span class="line">  callSite: <span class="type">CallSite</span>): <span class="type">ResultStage</span> = &#123;</span><br><span class="line"><span class="keyword">val</span> parents = getOrCreateParentStages(rdd, jobId)  <span class="comment">//</span></span><br><span class="line"><span class="keyword">val</span> id = nextStageId.getAndIncrement()</span><br><span class="line"><span class="keyword">val</span> stage = <span class="keyword">new</span> <span class="type">ResultStage</span>(id, rdd, func, partitions, parents, jobId, callSite)</span><br><span class="line">stageIdToStage(id) = stage</span><br><span class="line">updateJobIdStageIdMaps(jobId, stage)</span><br><span class="line">stage</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">……</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getOrCreateParentStages</span></span>(rdd: <span class="type">RDD</span>[_], firstJobId: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">Stage</span>] = &#123;</span><br><span class="line">getShuffleDependencies(rdd).map &#123; shuffleDep =&gt;</span><br><span class="line">  getOrCreateShuffleMapStage(shuffleDep, firstJobId)</span><br><span class="line">&#125;.toList</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">……</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">getShuffleDependencies</span></span>(</span><br><span class="line">  rdd: <span class="type">RDD</span>[_]): <span class="type">HashSet</span>[<span class="type">ShuffleDependency</span>[_, _, _]] = &#123;</span><br><span class="line"><span class="keyword">val</span> parents = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">ShuffleDependency</span>[_, _, _]]</span><br><span class="line"><span class="keyword">val</span> visited = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">RDD</span>[_]]</span><br><span class="line"><span class="keyword">val</span> waitingForVisit = <span class="keyword">new</span> <span class="type">Stack</span>[<span class="type">RDD</span>[_]]</span><br><span class="line">waitingForVisit.push(rdd)</span><br><span class="line"><span class="keyword">while</span> (waitingForVisit.nonEmpty) &#123;</span><br><span class="line">  <span class="keyword">val</span> toVisit = waitingForVisit.pop()</span><br><span class="line">  <span class="keyword">if</span> (!visited(toVisit)) &#123;</span><br><span class="line">    visited += toVisit</span><br><span class="line">    toVisit.dependencies.foreach &#123;</span><br><span class="line">      <span class="keyword">case</span> shuffleDep: <span class="type">ShuffleDependency</span>[_, _, _] =&gt;</span><br><span class="line">        parents += shuffleDep</span><br><span class="line">      <span class="keyword">case</span> dependency =&gt;</span><br><span class="line">        waitingForVisit.push(dependency.rdd)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">parents</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>RDD任务切分</strong></p>
<p>RDD任务切分中间分为：Application、Job、Stage和Task</p>
<ul>
<li><p>Application：初始化一个SparkContext即生成一个Application；</p>
</li>
<li><p>Job：一个Action算子就会生成一个Job；</p>
</li>
<li><p>Stage：Stage等于宽依赖(ShuffleDependency)的个数加1；</p>
</li>
<li><p>Task：一个Stage阶段中，最后一个RDD的分区个数就是Task的个数。</p>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> tasks: <span class="type">Seq</span>[<span class="type">Task</span>[_]] = <span class="keyword">try</span> &#123;</span><br><span class="line">  stage <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">      partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">        <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">        <span class="keyword">val</span> part = stage.rdd.partitions(id)</span><br><span class="line">        <span class="keyword">new</span> <span class="type">ShuffleMapTask</span>(stage.id, stage.latestInfo.attemptId,</span><br><span class="line">          taskBinary, part, locs, stage.latestInfo.taskMetrics, properties, <span class="type">Option</span>(jobId),</span><br><span class="line">          <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> stage: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">      partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">        <span class="keyword">val</span> p: <span class="type">Int</span> = stage.partitions(id)</span><br><span class="line">        <span class="keyword">val</span> part = stage.rdd.partitions(p)</span><br><span class="line">        <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">        <span class="keyword">new</span> <span class="type">ResultTask</span>(stage.id, stage.latestInfo.attemptId,</span><br><span class="line">          taskBinary, part, locs, id, properties, stage.latestInfo.taskMetrics,</span><br><span class="line">          <span class="type">Option</span>(jobId), <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId)</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">……</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> partitionsToCompute: <span class="type">Seq</span>[<span class="type">Int</span>] = stage.findMissingPartitions()</span><br><span class="line"></span><br><span class="line">……</span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">findMissingPartitions</span></span>(): <span class="type">Seq</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">mapOutputTrackerMaster</span><br><span class="line">  .findMissingPartitions(shuffleDep.shuffleId)</span><br><span class="line">  .getOrElse(<span class="number">0</span> until numPartitions)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="1-4-7-RDD持久化"><a href="#1-4-7-RDD持久化" class="headerlink" title="1.4.7 RDD持久化"></a>1.4.7 RDD持久化</h4><p><strong>RDD Cache缓存</strong></p>
<p>​	RDD通过Cache或者Persist方法将前面的计算结果缓存，默认情况下会把数据以缓存在JVM的堆内存中。但是并不是这两个方法被调用时立即缓存，而是触发后面的action算子时，该RDD将会被缓存在计算节点的内存中，并供后面重用。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cache操作会增加血缘关系，不改变原有的血缘关系</span></span><br><span class="line"><span class="comment">// 将血缘关系修改，添加一个和缓存相关的依赖关系</span></span><br><span class="line">println(wordToOneRdd.toDebugString)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 数据缓存。</span></span><br><span class="line">wordToOneRdd.cache()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 可以更改存储级别</span></span><br><span class="line"><span class="comment">//mapRdd.persist(StorageLevel.MEMORY_AND_DISK_2)</span></span><br></pre></td></tr></table></figure>

<p>​	存储级别</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StorageLevel</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> <span class="type">NONE</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="number">1</span>) <span class="comment">// 堆外内存，就是不受jvm管理的内存，是主动向操作系统申请的内存  </span></span><br></pre></td></tr></table></figure>

<p><img src="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/1662643880039.png" alt="1662643880039"></p>
<p>​	缓存有可能丢失，或者存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。</p>
<p>​	Spark会自动对一些Shuffle操作的中间数据做持久化操作(比如：reduceByKey)。这样做的目的是为了当一个节点Shuffle失败了避免重新计算整个输入。但是，在实际使用的时候，如果想重用数据，仍然建议调用persist或cache。<strong>cache内部使用的其实也是persist函数</strong></p>
<p>​	<strong>但是持久化的文件只能自己使用，使用完毕之后就会自动删除，因此如果有其它的app想要使用持久化的文件不能使用persist函数。此功能可以使用checkpoint操作。</strong></p>
<p>​	所谓的检查点其实就是通过将RDD中间结果写入磁盘</p>
<p>​	由于血缘依赖过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果检查点之后有节点出现问题，可以从检查点开始重做血缘，减少了开销。对RDD进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置检查点路径</span></span><br><span class="line">sc.setCheckpointDir(<span class="string">&quot;./checkpoint1&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一个RDD，读取指定位置文件:hello atguigu atguigu</span></span><br><span class="line"><span class="keyword">val</span> lineRdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;input/1.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 业务逻辑</span></span><br><span class="line"><span class="keyword">val</span> wordRdd: <span class="type">RDD</span>[<span class="type">String</span>] = lineRdd.flatMap(line =&gt; line.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> wordToOneRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Long</span>)] = wordRdd.map &#123;</span><br><span class="line">    word =&gt; &#123;</span><br><span class="line">        (word, <span class="type">System</span>.currentTimeMillis())</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 增加缓存,避免再重新跑一个job做checkpoint</span></span><br><span class="line">wordToOneRdd.cache()</span><br><span class="line"><span class="comment">// 数据检查点：针对wordToOneRdd做检查点计算</span></span><br><span class="line">wordToOneRdd.checkpoint()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 触发执行逻辑</span></span><br><span class="line">wordToOneRdd.collect().foreach(println)</span><br></pre></td></tr></table></figure>

<p><strong>缓存和检查点区别</strong></p>
<ul>
<li><p>Cache缓存只是将数据保存起来，不切断血缘依赖。Checkpoint检查点切断血缘依赖。</p>
</li>
<li><p>Cache缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint的数据通常存储在HDFS等容错、高可用的文件系统，可靠性高。</p>
</li>
<li><p>检查点为了数据的安全，会重新再执行一遍作业，所以会执行两次，但是这样效率比较低，因此建议checkpoint操作和cache操作一同使用，这样checkpoint的job只需从Cache缓存中读取数据即可，否则需要再从头计算一次RDD。</p>
</li>
</ul>
<h4 id="1-4-8-RDD分区器"><a href="#1-4-8-RDD分区器" class="headerlink" title="1.4.8 RDD分区器"></a>1.4.8 RDD分区器</h4><p>  Spark目前支持Hash分区和Range分区，和用户自定义分区。Hash分区为当前的默认分区。分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle后进入哪个分区，进而决定了Reduce的个数。</p>
<ul>
<li><p>只有Key-Value类型的RDD才有分区器，非Key-Value类型的RDD分区的值是None</p>
</li>
<li><p>每个RDD的分区ID范围：0 ~ (numPartitions - 1)，决定这个值是属于那个分区的。</p>
</li>
</ul>
<ol>
<li><strong>Hash分区</strong>：对于给定的key，计算其hashCode,并除以分区个数取余</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>(<span class="params">partitions: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">  require(partitions &gt;= <span class="number">0</span>, <span class="string">s&quot;Number of partitions (<span class="subst">$partitions</span>) cannot be negative.&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = partitions</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = key <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="literal">null</span> =&gt; <span class="number">0</span></span><br><span class="line">    <span class="keyword">case</span> _ =&gt; <span class="type">Utils</span>.nonNegativeMod(key.hashCode, numPartitions)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">equals</span></span>(other: <span class="type">Any</span>): <span class="type">Boolean</span> = other <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> h: <span class="type">HashPartitioner</span> =&gt;</span><br><span class="line">      h.numPartitions == numPartitions</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hashCode</span></span>: <span class="type">Int</span> = numPartitions</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><strong>Range分区</strong>：将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而且分区间有序</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RangePartitioner</span>[<span class="type">K</span> : <span class="type">Ordering</span> : <span class="type">ClassTag</span>, <span class="type">V</span>](<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    partitions: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    rdd: <span class="type">RDD</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]],</span></span></span><br><span class="line"><span class="params"><span class="class">    private var ascending: <span class="type">Boolean</span> = true</span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">Partitioner</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// We allow partitions = 0, which happens when sorting an empty RDD under the default settings.</span></span><br><span class="line">  require(partitions &gt;= <span class="number">0</span>, <span class="string">s&quot;Number of partitions cannot be negative but found <span class="subst">$partitions</span>.&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> ordering = implicitly[<span class="type">Ordering</span>[<span class="type">K</span>]]</span><br><span class="line"></span><br><span class="line">  <span class="comment">// An array of upper bounds for the first (partitions - 1) partitions</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> rangeBounds: <span class="type">Array</span>[<span class="type">K</span>] = &#123;</span><br><span class="line">  ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = rangeBounds.length + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> binarySearch: ((<span class="type">Array</span>[<span class="type">K</span>], <span class="type">K</span>) =&gt; <span class="type">Int</span>) = <span class="type">CollectionsUtils</span>.makeBinarySearch[<span class="type">K</span>]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> k = key.asInstanceOf[<span class="type">K</span>]</span><br><span class="line">    <span class="keyword">var</span> partition = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> (rangeBounds.length &lt;= <span class="number">128</span>) &#123;</span><br><span class="line">      <span class="comment">// If we have less than 128 partitions naive search</span></span><br><span class="line">      <span class="keyword">while</span> (partition &lt; rangeBounds.length &amp;&amp; ordering.gt(k, rangeBounds(partition))) &#123;</span><br><span class="line">        partition += <span class="number">1</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Determine which binary search method to use only once.</span></span><br><span class="line">      partition = binarySearch(rangeBounds, k)</span><br><span class="line">      <span class="comment">// binarySearch either returns the match location or -[insertion point]-1</span></span><br><span class="line">      <span class="keyword">if</span> (partition &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        partition = -partition<span class="number">-1</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (partition &gt; rangeBounds.length) &#123;</span><br><span class="line">        partition = rangeBounds.length</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (ascending) &#123;</span><br><span class="line">      partition</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      rangeBounds.length - partition</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">equals</span></span>(other: <span class="type">Any</span>): <span class="type">Boolean</span> = other <span class="keyword">match</span> &#123;</span><br><span class="line">  ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hashCode</span></span>(): <span class="type">Int</span> = &#123;</span><br><span class="line">  ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@throws</span>(classOf[<span class="type">IOException</span>])</span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">writeObject</span></span>(out: <span class="type">ObjectOutputStream</span>): <span class="type">Unit</span> = <span class="type">Utils</span>.tryOrIOException &#123;</span><br><span class="line">  ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@throws</span>(classOf[<span class="type">IOException</span>])</span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">readObject</span></span>(in: <span class="type">ObjectInputStream</span>): <span class="type">Unit</span> = <span class="type">Utils</span>.tryOrIOException &#123;</span><br><span class="line">  ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>如何定义一个自己的分区器</strong></p>
<p>自定义分区器</p>
<ol>
<li>继承Partitioner</li>
<li>重写方法</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.bigdata.spark.rdd.part</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.<span class="type">StorageLevel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">Partitioner</span>, <span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark01_RDD_Part</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;WordCount&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = sc.makeRDD(</span><br><span class="line">            <span class="type">List</span>(</span><br><span class="line">                (<span class="string">&quot;nba&quot;</span>, <span class="string">&quot;xxxxxx&quot;</span>),</span><br><span class="line">                (<span class="string">&quot;cba&quot;</span>, <span class="string">&quot;11111&quot;</span>),</span><br><span class="line">                (<span class="string">&quot;nba&quot;</span>, <span class="string">&quot;yyyyy&quot;</span>),</span><br><span class="line">                (<span class="string">&quot;wnba&quot;</span>, <span class="string">&quot;22222&quot;</span>)</span><br><span class="line">            ),<span class="number">2</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = lines.partitionBy(<span class="keyword">new</span> <span class="type">MyPartitioner</span>())</span><br><span class="line"></span><br><span class="line">        rdd2.saveAsTextFile(<span class="string">&quot;output&quot;</span>)</span><br><span class="line"></span><br><span class="line">        sc.stop()</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 自定义分区器</span></span><br><span class="line">    <span class="comment">// 1. 继承Partitioner</span></span><br><span class="line">    <span class="comment">// 2. 重写方法</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">        <span class="comment">// TODO 分区数量</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = &#123;</span><br><span class="line">            <span class="number">3</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 根据数据的key返回所在的分区编号，从0开始</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">            key <span class="keyword">match</span> &#123;</span><br><span class="line">                <span class="keyword">case</span> <span class="string">&quot;nba&quot;</span> =&gt; <span class="number">0</span></span><br><span class="line">                <span class="keyword">case</span> <span class="string">&quot;cba&quot;</span> =&gt; <span class="number">1</span></span><br><span class="line">                <span class="keyword">case</span> <span class="string">&quot;wnba&quot;</span> =&gt; <span class="number">2</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>​	定义一个自己的分区器需要重写四个方法，其中有两个方法的目的主要是为了防止前一次的RDD和后一次的RDD做了相同的shuffle分组操作，这样会极大的耗费性能，因此重写equals方法之后，会进行判断，如果两次分区器相等，那么就不再执行shuffle此操作，这样可以提升性能。这两个方法非必要，但建议重写。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.bigdata.spark.rdd.part</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">HashPartitioner</span>, <span class="type">Partitioner</span>, <span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark02_RDD_Part</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;WordCount&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = sc.makeRDD(</span><br><span class="line">            <span class="type">List</span>(</span><br><span class="line">                (<span class="string">&quot;nba&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                (<span class="string">&quot;cba&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                (<span class="string">&quot;nba&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                (<span class="string">&quot;wnba&quot;</span>, <span class="number">1</span>)</span><br><span class="line">            ),<span class="number">2</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="comment">//        val rdd1: RDD[(String, Int)] = lines.reduceByKey(_ + _)</span></span><br><span class="line"><span class="comment">//        val rdd2: RDD[(String, Int)] = rdd1.reduceByKey(_ + _)</span></span><br><span class="line">         <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = lines.partitionBy(<span class="keyword">new</span> <span class="type">MyPartitioner</span>())</span><br><span class="line">        <span class="comment">// 当rdd1有分区器的时候，会判断本次和前一次的分区器是否一样，如果一样就没有必要再进行shuffle操作</span></span><br><span class="line">         <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = rdd1.partitionBy(<span class="keyword">new</span> <span class="type">MyPartitioner</span>())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        sc.stop()</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 自定义分区器</span></span><br><span class="line">    <span class="comment">// 1. 继承Partitioner</span></span><br><span class="line">    <span class="comment">// 2. 重写方法(2 + 2)</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">        <span class="comment">// TODO 分区数量</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = &#123;</span><br><span class="line">            <span class="number">3</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 根据数据的key返回所在的分区编号，从0开始</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">            key <span class="keyword">match</span> &#123;</span><br><span class="line">                <span class="keyword">case</span> <span class="string">&quot;nba&quot;</span> =&gt; <span class="number">0</span></span><br><span class="line">                <span class="keyword">case</span> <span class="string">&quot;cba&quot;</span> =&gt; <span class="number">1</span></span><br><span class="line">                <span class="keyword">case</span> <span class="string">&quot;wnba&quot;</span> =&gt; <span class="number">2</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">equals</span></span>(other: <span class="type">Any</span>): <span class="type">Boolean</span> = other <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> h: <span class="type">MyPartitioner</span> =&gt;</span><br><span class="line">                h.numPartitions == numPartitions</span><br><span class="line">            <span class="keyword">case</span> _ =&gt;</span><br><span class="line">                <span class="literal">false</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hashCode</span></span>: <span class="type">Int</span> = numPartitions</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="1-4-9-RDD文件读取与保存"><a href="#1-4-9-RDD文件读取与保存" class="headerlink" title="1.4.9 RDD文件读取与保存"></a>1.4.9 RDD文件读取与保存</h4><p>​	Spark的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统。</p>
<ol>
<li><p>文件格式分为：text文件、csv文件、sequence文件以及Object文件；</p>
</li>
<li><p>文件系统分为：本地文件系统、HDFS、HBASE以及数据库</p>
</li>
</ol>
<p>  <strong>text文件</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 读取输入文件</span></span><br><span class="line"><span class="keyword">val</span> inputRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;input/1.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 保存数据</span></span><br><span class="line">inputRDD.saveAsTextFile(<span class="string">&quot;output&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>sequence文件</strong></p>
<p>​	SequenceFile文件是<a target="_blank" rel="noopener" href="http://lib.csdn.net/base/hadoop">Hadoop</a>用来存储二进制形式的key-value对而设计的一种平面文件(Flat File)。在SparkContext中，可以调用sequenceFile[keyClass,valueClass](path)。</p>
<p>​	读取的时候需要注意要指定key和value的类型是什么</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 保存数据为SequenceFile</span></span><br><span class="line">dataRDD.saveAsSequenceFile(<span class="string">&quot;output&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 读取SequenceFile文件</span></span><br><span class="line">sc.sequenceFile[<span class="type">Int</span>,<span class="type">Int</span>](<span class="string">&quot;output&quot;</span>).collect().foreach(println)</span><br></pre></td></tr></table></figure>

<p><strong>object文件</strong></p>
<p>​	对象文件是将对象序列化后保存的文件，采用Java的序列化机制。可以通过objectFile<a href="path">T: ClassTag</a>函数接收一个路径，读取对象文件，返回对应的RDD，也可以通过调用saveAsObjectFile()实现对对象文件的输出。因为是序列化所以要指定类型</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 保存数据</span></span><br><span class="line">dataRDD.saveAsObjectFile(<span class="string">&quot;output&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 读取数据</span></span><br><span class="line">sc.objectFile[<span class="type">Int</span>](<span class="string">&quot;output&quot;</span>).collect().foreach(println)</span><br></pre></td></tr></table></figure>

<h2 id="2-累加器"><a href="#2-累加器" class="headerlink" title="2. 累加器"></a>2. 累加器</h2><p><strong>实现原理</strong></p>
<p>​	累加器用来把Executor端变量信息聚合到Driver端。在Driver程序中定义的变量，在Executor端的每个Task都会得到这个变量的一份新的副本，每个task更新这些副本的值后，传回Driver端进行merge。</p>
<p><strong>默认累加器</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line"><span class="comment">// 声明累加器</span></span><br><span class="line"><span class="keyword">var</span> sum = sc.longAccumulator(<span class="string">&quot;sum&quot;</span>);</span><br><span class="line">rdd.foreach(</span><br><span class="line">  num =&gt; &#123;</span><br><span class="line">    <span class="comment">// 使用累加器</span></span><br><span class="line">    sum.add(num)</span><br><span class="line">  &#125;</span><br><span class="line">)</span><br><span class="line"><span class="comment">// 获取累加器的值</span></span><br><span class="line">println(<span class="string">&quot;sum = &quot;</span> + sum.value)</span><br></pre></td></tr></table></figure>

<p><strong>自定义累加器</strong></p>
<p>实现wordcount</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.bigdata.spark.rdd.acc</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.util.<span class="type">AccumulatorV2</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark03_Acc</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;WordCount&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(</span><br><span class="line">            <span class="type">List</span>(</span><br><span class="line">                <span class="string">&quot;scala&quot;</span>,</span><br><span class="line">                <span class="string">&quot;scala&quot;</span>,</span><br><span class="line">                <span class="string">&quot;scala&quot;</span>,</span><br><span class="line">                <span class="string">&quot;scala&quot;</span>,</span><br><span class="line">                <span class="string">&quot;scala&quot;</span>,</span><br><span class="line">                <span class="string">&quot;scala&quot;</span>,</span><br><span class="line">                <span class="string">&quot;spark&quot;</span>,</span><br><span class="line">                <span class="string">&quot;spark&quot;</span>,</span><br><span class="line">                <span class="string">&quot;spark&quot;</span>,</span><br><span class="line">                <span class="string">&quot;spark&quot;</span></span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 创建累加器</span></span><br><span class="line">        <span class="keyword">val</span> acc = <span class="keyword">new</span> <span class="type">WordCountAccumulator</span>()</span><br><span class="line">        <span class="comment">// TODO 向Spark进行注册</span></span><br><span class="line">        sc.register(acc, <span class="string">&quot;wordCount&quot;</span>)</span><br><span class="line"></span><br><span class="line">        rdd.foreach(</span><br><span class="line">            word =&gt; &#123;</span><br><span class="line">                <span class="comment">// TODO 将单词放入到累加器中</span></span><br><span class="line">                acc.add(word)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 获取累加器的累加结果</span></span><br><span class="line">        println(acc.value)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        sc.stop()</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 自定义数据累加器</span></span><br><span class="line">    <span class="comment">// 1. 继承AccumulatorV2</span></span><br><span class="line">    <span class="comment">// 2. 定义泛型</span></span><br><span class="line">    <span class="comment">//    IN : String</span></span><br><span class="line">    <span class="comment">//    OUT : Map[K, V]</span></span><br><span class="line">    <span class="comment">// 3. 重写方法（3 + 3）</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">WordCountAccumulator</span> <span class="keyword">extends</span> <span class="title">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>]] </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">val</span> wcMap = mutable.<span class="type">Map</span>[<span class="type">String</span>,<span class="type">Int</span>]()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 判断累加器是否为初始状态</span></span><br><span class="line">        <span class="comment">// copyAndReset must return a zero value copy</span></span><br><span class="line">        <span class="comment">// TODO 3. true</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">            wcMap.isEmpty</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 1.</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>]] = &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">WordCountAccumulator</span>()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 重置累加器</span></span><br><span class="line">        <span class="comment">// TODO 2.</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">            wcMap.clear()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 从外部向累加器中添加数据</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(word: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">            <span class="keyword">val</span> oldCnt = wcMap.getOrElse(word, <span class="number">0</span>)</span><br><span class="line">            wcMap.update(word, oldCnt + <span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 合并两个累加器的结果</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">            other.value.foreach &#123;</span><br><span class="line">                <span class="keyword">case</span> (word, cnt) =&gt; &#123;</span><br><span class="line">                    <span class="keyword">val</span> oldCnt = wcMap.getOrElse(word, <span class="number">0</span>)</span><br><span class="line">                    wcMap.update( word, oldCnt + cnt )</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将结果返回到外部</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>] = wcMap</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="3-广播变量"><a href="#3-广播变量" class="headerlink" title="3. 广播变量"></a>3. 广播变量</h2><p>​	广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个Spark操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark会为每个任务分别发送。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">List</span>( (<span class="string">&quot;a&quot;</span>,<span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;d&quot;</span>, <span class="number">4</span>) ),<span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>( (<span class="string">&quot;a&quot;</span>,<span class="number">4</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">6</span>), (<span class="string">&quot;d&quot;</span>, <span class="number">7</span>) )</span><br><span class="line"><span class="comment">// 声明广播变量</span></span><br><span class="line"><span class="keyword">val</span> broadcast: <span class="type">Broadcast</span>[<span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)]] = sc.broadcast(list)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> resultRDD: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>))] = rdd1.map &#123;</span><br><span class="line">  <span class="keyword">case</span> (key, num) =&gt; &#123;</span><br><span class="line">    <span class="keyword">var</span> num2 = <span class="number">0</span></span><br><span class="line">    <span class="comment">// 使用广播变量</span></span><br><span class="line">    <span class="keyword">for</span> ((k, v) &lt;- broadcast.value) &#123;</span><br><span class="line">      <span class="keyword">if</span> (k == key) &#123;</span><br><span class="line">        num2 = v</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    (key, (num, num2))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Xiang Liu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/">http://example.com/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">The Xiang Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/RDD/">RDD</a><a class="post-meta__tags" href="/tags/%E8%A1%8C%E5%8A%A8%E7%AE%97%E5%AD%90/">行动算子</a><a class="post-meta__tags" href="/tags/%E8%BD%AC%E6%8D%A2%E7%AE%97%E5%AD%90/">转换算子</a><a class="post-meta__tags" href="/tags/map/">map</a><a class="post-meta__tags" href="/tags/groupBy/">groupBy</a><a class="post-meta__tags" href="/tags/shuffle/">shuffle</a><a class="post-meta__tags" href="/tags/RDD%E7%9A%84%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5/">RDD的分区策略</a><a class="post-meta__tags" href="/tags/RDD%E7%9A%84%E5%88%9B%E5%BB%BA/">RDD的创建</a><a class="post-meta__tags" href="/tags/flatMap/">flatMap</a><a class="post-meta__tags" href="/tags/glom/">glom</a><a class="post-meta__tags" href="/tags/aggregateByKey/">aggregateByKey</a><a class="post-meta__tags" href="/tags/reduceByKey/">reduceByKey</a><a class="post-meta__tags" href="/tags/combineByKey/">combineByKey</a><a class="post-meta__tags" href="/tags/foldByKey/">foldByKey</a><a class="post-meta__tags" href="/tags/%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB/">血缘关系</a><a class="post-meta__tags" href="/tags/presist/">presist</a><a class="post-meta__tags" href="/tags/cache/">cache</a><a class="post-meta__tags" href="/tags/checkpoint/">checkpoint</a><a class="post-meta__tags" href="/tags/%E6%8C%81%E4%B9%85%E5%8C%96/">持久化</a><a class="post-meta__tags" href="/tags/%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB/">依赖关系</a><a class="post-meta__tags" href="/tags/%E5%88%86%E5%8C%BA%E5%99%A8/">分区器</a><a class="post-meta__tags" href="/tags/%E5%BA%8F%E5%88%97%E5%8C%96/">序列化</a></div><div class="post_share"><div class="social-share" data-image="/../image/bg.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/09/08/Spark/Spark%E5%B0%8F%E6%A1%88%E4%BE%8B/"><img class="prev-cover" src="/../image/bg.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Spark一个小案例</div></div></a></div><div class="next-post pull-right"><a href="/2022/09/06/Spark/spark%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84/"><img class="next-cover" src="/../image/bg.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Spark运行架构</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/09/03/scala/scala%E7%AE%80%E4%BB%8B/" title="Scala概述"><img class="cover" src="/../image/bg.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-03</div><div class="title">Scala概述</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/./image/title.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Xiang Liu</div><div class="author-info__description">欢迎访问我的博客</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">28</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">191</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/theXiangCode" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://mail.qq.com/" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">这几天心理颇不宁静</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text"></span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-text">简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-RDD"><span class="toc-text">1.RDD</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E4%BB%80%E4%B9%88%E6%98%AFRDD%EF%BC%9F"><span class="toc-text">1.1 什么是RDD？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E6%A0%B8%E5%BF%83%E5%B1%9E%E6%80%A7"><span class="toc-text">1.2 核心属性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E6%89%A7%E8%A1%8C%E5%8E%9F%E7%90%86"><span class="toc-text">1.3 执行原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A8%8B"><span class="toc-text">1.4 基础编程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-1-RDD%E7%9A%84%E5%9B%9B%E7%A7%8D%E5%88%9B%E5%BB%BA%E6%96%B9%E5%BC%8F"><span class="toc-text">1.4.1 RDD的四种创建方式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-2-RDD%E7%9A%84%E5%88%86%E5%8C%BA%E5%92%8C%E5%B9%B6%E8%A1%8C%E5%BA%A6"><span class="toc-text">1.4.2 RDD的分区和并行度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-3-RDD%E8%BD%AC%E6%8D%A2%E7%AE%97%E5%AD%90"><span class="toc-text">1.4.3 RDD转换算子</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-1-map"><span class="toc-text">1.4.3.1 map</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-2-mapPartitions"><span class="toc-text">1.4.3.2 mapPartitions</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-3-mapPartitionsWithIndex"><span class="toc-text">1.4.3.3 mapPartitionsWithIndex</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-4-flatMap"><span class="toc-text">1.4.3.4 flatMap</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-5-glom"><span class="toc-text">1.4.3.5 glom</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-6-groupBy-shuffle"><span class="toc-text">1.4.3.6 groupBy(shuffle)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-7-filter"><span class="toc-text">1.4.3.7 filter</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-8-sample"><span class="toc-text">1.4.3.8 sample</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-9-distinct-shuffle"><span class="toc-text">1.4.3.9 distinct (shuffle)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-10-coalesce-shuffle"><span class="toc-text">1.4.3.10 coalesce(shuffle)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-11-repartition-shuffle"><span class="toc-text">1.4.3.11 repartition (shuffle)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-12-sortBy-shuffle"><span class="toc-text">1.4.3.12 sortBy (shuffle)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-13-intersection"><span class="toc-text">1.4.3.13 intersection</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-14-union"><span class="toc-text">1.4.3.14 union</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-15-subtract"><span class="toc-text">1.4.3.15 subtract</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-16-zip"><span class="toc-text">1.4.3.16 zip</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-17-partitionBy"><span class="toc-text">1.4.3.17 partitionBy</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-18-reduceByKey"><span class="toc-text">1.4.3.18  reduceByKey</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-19-groupByKey"><span class="toc-text">1.4.3.19 groupByKey</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-20-aggregateByKey"><span class="toc-text">1.4.3.20 aggregateByKey</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-21-foldByKey"><span class="toc-text">1.4.3.21 foldByKey</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-22-combineByKey"><span class="toc-text">1.4.3.22 combineByKey</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-23-sortByKey"><span class="toc-text">1.4.3.23 sortByKey</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-24-join"><span class="toc-text">1.4.3.24 join</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-25-leftOuterJoin"><span class="toc-text">1.4.3.25 leftOuterJoin</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-3-26-cogroup"><span class="toc-text">1.4.3.26 cogroup</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-4-RDD%E8%A1%8C%E5%8A%A8%E7%AE%97%E5%AD%90"><span class="toc-text">1.4.4 RDD行动算子</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-4-1-reduce"><span class="toc-text">1.4.4.1 reduce</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-4-2-collect"><span class="toc-text">1.4.4.2 collect</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-4-3-count"><span class="toc-text">1.4.4.3 count</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-4-4-first"><span class="toc-text">1.4.4.4 first</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-4-5-take"><span class="toc-text">1.4.4.5 take</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-4-6-takeOrdered"><span class="toc-text">1.4.4.6 takeOrdered</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-4-7-aggregate"><span class="toc-text">1.4.4.7 aggregate</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-4-8-fold"><span class="toc-text">1.4.4.8 fold</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-4-9-countByKey"><span class="toc-text">1.4.4.9 countByKey</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-4-10-save%E7%9B%B8%E5%85%B3%E7%AE%97%E5%AD%90"><span class="toc-text">1.4.4.10 save相关算子</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-4-11-foreach"><span class="toc-text">1.4.4.11 foreach</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-5-RDD%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-text">1.4.5 RDD序列化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-6-RDD%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB"><span class="toc-text">1.4.6 RDD依赖关系</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-7-RDD%E6%8C%81%E4%B9%85%E5%8C%96"><span class="toc-text">1.4.7 RDD持久化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-8-RDD%E5%88%86%E5%8C%BA%E5%99%A8"><span class="toc-text">1.4.8 RDD分区器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-9-RDD%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96%E4%B8%8E%E4%BF%9D%E5%AD%98"><span class="toc-text">1.4.9 RDD文件读取与保存</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="toc-text">2. 累加器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="toc-text">3. 广播变量</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/09/10/Java/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86/" title="Java基本知识点介绍"><img src="/../image/bg.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Java基本知识点介绍"/></a><div class="content"><a class="title" href="/2022/09/10/Java/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86/" title="Java基本知识点介绍">Java基本知识点介绍</a><time datetime="2022-09-10T12:44:35.832Z" title="发表于 2022-09-10 20:44:35">2022-09-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86/" title="计算机网络知识点介绍"><img src="/../image/bg.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机网络知识点介绍"/></a><div class="content"><a class="title" href="/2022/09/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86/" title="计算机网络知识点介绍">计算机网络知识点介绍</a><time datetime="2022-09-10T12:26:42.061Z" title="发表于 2022-09-10 20:26:42">2022-09-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/10/Spark/Spark%E6%BA%90%E7%A0%81%E5%86%85%E6%A0%B8/" title="Spark内核"><img src="/../image/bg.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Spark内核"/></a><div class="content"><a class="title" href="/2022/09/10/Spark/Spark%E6%BA%90%E7%A0%81%E5%86%85%E6%A0%B8/" title="Spark内核">Spark内核</a><time datetime="2022-09-10T02:54:42.307Z" title="发表于 2022-09-10 10:54:42">2022-09-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/08/Spark/Spark%E5%B0%8F%E6%A1%88%E4%BE%8B/" title="Spark一个小案例"><img src="/../image/bg.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Spark一个小案例"/></a><div class="content"><a class="title" href="/2022/09/08/Spark/Spark%E5%B0%8F%E6%A1%88%E4%BE%8B/" title="Spark一个小案例">Spark一个小案例</a><time datetime="2022-09-08T15:24:18.328Z" title="发表于 2022-09-08 23:24:18">2022-09-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/" title="Spark核心编程"><img src="/../image/bg.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Spark核心编程"/></a><div class="content"><a class="title" href="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/" title="Spark核心编程">Spark核心编程</a><time datetime="2022-09-06T11:27:39.924Z" title="发表于 2022-09-06 19:27:39">2022-09-06</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/../image/bg.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2022 <i style="color:#FF6A6A;animation: announ_animation 0.8s linear infinite;" class="fa fa-heartbeat"></i> Xiang Liu</div><div class="footer_custom_text">兽人永不为奴!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><script src="https://gcore.jsdelivr.net/gh/xiabo2/CDN@latest/fishes.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>