<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Hive概念介绍 | The Xiang Blog</title><meta name="keywords" content="UDF,UDTF,分区表,分桶表,开窗函数,Hive"><meta name="author" content="Xiang Liu"><meta name="copyright" content="Xiang Liu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="介绍了Hive中的各种概念">
<meta property="og:type" content="article">
<meta property="og:title" content="Hive概念介绍">
<meta property="og:url" content="http://example.com/2022/08/30/Hive/hive%E6%A6%82%E8%BF%B0/index.html">
<meta property="og:site_name" content="The Xiang Blog">
<meta property="og:description" content="介绍了Hive中的各种概念">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/image/bg.jpg">
<meta property="article:published_time" content="2022-08-30T15:49:27.546Z">
<meta property="article:modified_time" content="2022-08-31T09:59:56.624Z">
<meta property="article:author" content="Xiang Liu">
<meta property="article:tag" content="UDF">
<meta property="article:tag" content="UDTF">
<meta property="article:tag" content="分区表">
<meta property="article:tag" content="分桶表">
<meta property="article:tag" content="开窗函数">
<meta property="article:tag" content="Hive">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/image/bg.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2022/08/30/Hive/hive%E6%A6%82%E8%BF%B0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Hive概念介绍',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2022-08-31 17:59:56'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/./image/title.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">28</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">191</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 总览</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/../image/bg.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">The Xiang Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 总览</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Hive概念介绍</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-08-30T15:49:27.546Z" title="发表于 2022-08-30 23:49:27">2022-08-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-08-31T09:59:56.624Z" title="更新于 2022-08-31 17:59:56">2022-08-31</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Hive/">Hive</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>34分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Hive概念介绍"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="1-Hive的几种启动方式"><a href="#1-Hive的几种启动方式" class="headerlink" title="1. Hive的几种启动方式"></a>1. Hive的几种启动方式</h2><h3 id="1-1-命令行模式"><a href="#1-1-命令行模式" class="headerlink" title="1.1 命令行模式"></a>1.1 命令行模式</h3><p>先启动hadoop集群之后，然后就可以直接在命令行启动Hive</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive</span><br></pre></td></tr></table></figure>

<h3 id="1-2-使用元数据服务的方式访问Hive"><a href="#1-2-使用元数据服务的方式访问Hive" class="headerlink" title="1.2 使用元数据服务的方式访问Hive"></a>1.2 使用元数据服务的方式访问Hive</h3><p><strong>1）在hive-site.xml文件中添加如下配置信息</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 指定存储元数据要连接的地址 --&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">     &lt;name&gt;hive.metastore.uris&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;thrift://hadoop102:9083&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p><strong>2）启动metastore</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop202 hive]$ hive --service metastore</span><br><span class="line">2020-04-24 16:58:08: Starting Hive Metastore Server</span><br></pre></td></tr></table></figure>

<p>注意: 启动后窗口不能再操作，需打开一个新的shell窗口做别的操作</p>
<p><strong>3）启动hive</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop202 hive]$ bin/hive</span><br></pre></td></tr></table></figure>



<p><strong>好奇一个问题：我们为什么要配置这个metastore服务？</strong></p>
<p>首先，什么是元数据：<a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=%E5%85%83%E6%95%B0%E6%8D%AE&spm=1001.2101.3001.7020">元数据</a>包含用Hive创建的database、table等的元信息。元数据存储在关系型数据库中。如Derby、MySQL等。</p>
<p>那么，配置的metastore有什么用：<strong>客户端连接metastore服务，metastore再去连接MySQL数据库来存取元数据。有了metastore服务，就可以有多个客户端同时连接，而且这些客户端不需要知道MySQL数据库的用户名和密码，只需要连接metastore 服务即可。</strong></p>
<h3 id="1-3-使用JDBC方式来访问Hive"><a href="#1-3-使用JDBC方式来访问Hive" class="headerlink" title="1.3 使用JDBC方式来访问Hive"></a>1.3 使用JDBC方式来访问Hive</h3><ol>
<li><strong>在hive-site.xml文件中添加如下配置信息</strong></li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 指定hiveserver2连接的host --&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">     &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;hadoop102&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"></span><br><span class="line"> &lt;!-- 指定hiveserver2连接的端口号 --&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">     &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;10000&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><strong>启动hiveserver2</strong></li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive --service hiveserver2</span><br></pre></td></tr></table></figure>

<ol start="3">
<li><strong>启动beeline客户端</strong></li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/beeline -u jdbc:hive2://hadoop102:10000 -n atguigu</span><br></pre></td></tr></table></figure>

<h3 id="1-4-hiveserver2和metastore的联系"><a href="#1-4-hiveserver2和metastore的联系" class="headerlink" title="1.4 hiveserver2和metastore的联系"></a>1.4 hiveserver2和metastore的联系</h3><p>hiveServer2和metaStore其实都是hive本身带的组件，那么两者究竟有什么不同呢？</p>
<ol>
<li><p>metaStore：hive的metaStore提供的是一个服务，而这个服务就是将hive的元数据暴露出去，而不是需要通过对hive元数据库mysql的访问才能拿到hive的元数据信息;metastore服务实际上就是一种thrift服务，通过它我们可以获取到hive原数据，并且通过thrift获取原数据的方式，屏蔽了数据库访问需要驱动，url，用户名，密码等等细节</p>
</li>
<li><p>hiveServer2: HiveServer2（HS2）是一个服务端接口，使远程客户端可以执行对Hive的查询并返回结果。目前基于Thrift RPC的实现是HiveServer的改进版本，并支持多客户端并发和身份验证, 启动hiveServer2服务后，就可以使用jdbc，odbc，或者thrift的方式连接</p>
</li>
</ol>
<p><strong>总结：</strong></p>
<ol>
<li>HiveServer2和MetaStore本质上都是Thrift Service，虽然可以启动在同一个进程内，但不建议这么做。建议是拆成不同的服务进程来启动。</li>
<li>一般来讲，我们认为HiveServer2是用来提交查询的，也就是用来访问数据的。而MetaStore才是用来访问元数据的。<strong>在我的认知下，其实就是metaStore用来访问元数据，而hiveserver只是在metaStore的上层又添加了一层功能（用来让用户可以使用jdbc等方式连接）</strong></li>
</ol>
<h3 id="1-5-总结"><a href="#1-5-总结" class="headerlink" title="1.5 总结"></a>1.5 总结</h3><p>1）hive有2种客户端：hive客户端和beeline客户端，beeline客户端是通过hiveserver2服务以JDBC的方式连接hive客户端。<br>2）hive作为服务端有2个后台服务：metastore服务，hiveserver2服务。<br>3）hive连接元数据有2种方式：直接连接和metastore服务连接。<br>4）如果配置了hive.metastore.uris参数，必须启动metastore服务才能连接元数据；如果没有配置可以直接连接元数据。<br>5）hive本身既是客户端，又是服务端。</p>
<h2 id="2-Hive的数据类型"><a href="#2-Hive的数据类型" class="headerlink" title="2.Hive的数据类型"></a>2.Hive的数据类型</h2><h3 id="2-1Hive的基本数据类型"><a href="#2-1Hive的基本数据类型" class="headerlink" title="2.1Hive的基本数据类型"></a>2.1Hive的基本数据类型</h3><table>
<thead>
<tr>
<th>Hive数据类型</th>
<th>Java数据类型</th>
<th>长度</th>
<th>例子</th>
</tr>
</thead>
<tbody><tr>
<td>TINYINT</td>
<td>byte</td>
<td>1byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>SMALINT</td>
<td>short</td>
<td>2byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>INT</td>
<td>int</td>
<td>4byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BIGINT</td>
<td>long</td>
<td>8byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BOOLEAN</td>
<td>boolean</td>
<td>布尔类型，true或者false</td>
<td>TRUE  FALSE</td>
</tr>
<tr>
<td>FLOAT</td>
<td>float</td>
<td>单精度浮点数</td>
<td>3.14159</td>
</tr>
<tr>
<td>DOUBLE</td>
<td>double</td>
<td>双精度浮点数</td>
<td>3.14159</td>
</tr>
<tr>
<td>STRING</td>
<td>string</td>
<td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td>
<td>‘now is the time’   “for all good men”</td>
</tr>
<tr>
<td>TIMESTAMP</td>
<td></td>
<td>时间类型</td>
<td></td>
</tr>
<tr>
<td>BINARY</td>
<td></td>
<td>字节数组</td>
<td></td>
</tr>
</tbody></table>
<p>​	其中的string相当于数据库中的varchar类型，改类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。</p>
<h3 id="2-2-集合数据类型"><a href="#2-2-集合数据类型" class="headerlink" title="2.2 集合数据类型"></a>2.2 集合数据类型</h3><table>
<thead>
<tr>
<th>数据类型</th>
<th>描述</th>
<th>语法示例</th>
</tr>
</thead>
<tbody><tr>
<td>STRUCT</td>
<td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last   STRING},那么第1个元素可以通过字段.first来引用。</td>
<td>struct()   例如struct&lt;street:string, city:string&gt;</td>
</tr>
<tr>
<td>MAP</td>
<td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td>
<td>map()   例如map&lt;string, int&gt;</td>
</tr>
<tr>
<td>ARRAY</td>
<td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td>
<td>Array()   例如array<string></td>
</tr>
</tbody></table>
<p>​	Hive有三种复杂数据类型ARRAY、MAP 和STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</p>
<p><strong>实例展示：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;name&quot;: &quot;songsong&quot;,</span><br><span class="line">    &quot;friends&quot;: [&quot;bingbing&quot; , &quot;lili&quot;] ,       //列表Array, </span><br><span class="line">    &quot;children&quot;: &#123;                      //键值Map,</span><br><span class="line">        &quot;xiao song&quot;: 19 ,</span><br><span class="line">        &quot;xiaoxiao song&quot;: 18</span><br><span class="line">    &#125;</span><br><span class="line">    &quot;address&quot;: &#123;                      //结构Struct,</span><br><span class="line">        &quot;street&quot;: &quot;hui long guan&quot; ,</span><br><span class="line">        &quot;city&quot;: &quot;beijing&quot; </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="3-Hive创建表格"><a href="#3-Hive创建表格" class="headerlink" title="3.Hive创建表格"></a>3.Hive创建表格</h2><p>建表语法</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name </span><br><span class="line">[(col_name data_type [COMMENT col_comment], ...)] </span><br><span class="line">[COMMENT table_comment] </span><br><span class="line">[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] </span><br><span class="line">[CLUSTERED BY (col_name, col_name, ...) </span><br><span class="line">[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] </span><br><span class="line">[ROW FORMAT row_format] </span><br><span class="line">[STORED AS file_format] </span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[TBLPROPERTIES (property_name=property_value, ...)]</span><br><span class="line">[AS select_statement]</span><br></pre></td></tr></table></figure>

<p>（1）CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。</p>
<p>（2）EXTERNAL关键字可以让用户创建一个外部表，在建表的同时可以指定一个指向实际数据的路径（LOCATION），在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</p>
<p>（3）COMMENT：为表和列添加注释。</p>
<p>（4）PARTITIONED BY创建分区表</p>
<p>（5）CLUSTERED BY创建分桶表</p>
<p>（6）SORTED BY不常用，对桶中的一个或多个列另外排序</p>
<p>（7）ROW FORMAT </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char]</span><br><span class="line">        [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] </span><br><span class="line">   | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]</span><br></pre></td></tr></table></figure>

<p>用户在建表的时候可以自定义SerDe或者使用自带的SerDe。如果没有指定ROW FORMAT 或者ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。</p>
<p>SerDe是Serialize&#x2F;Deserilize的简称， hive使用Serde进行行对象的序列与反序列化。</p>
<p>（8）STORED AS指定存储文件类型</p>
<p>常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）</p>
<p>如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。</p>
<p>（9）LOCATION ：指定表在HDFS上的存储位置(这样其实就是把HDFS中的结构化文件映射到表格中来)，如果在这里没有指定文件路径，那么我们还需要后续指定，或者手动添加数据（一般不常用）。</p>
<p>（10）AS：后跟查询语句，根据查询结果创建表。</p>
<p>（11）LIKE允许用户复制现有的表结构，但是不复制数据。</p>
<h3 id="3-1-内部表"><a href="#3-1-内部表" class="headerlink" title="3.1 内部表"></a>3.1 内部表</h3><p>​	默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive会（或多或少地）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，&#x2F;user&#x2F;hive&#x2F;warehouse)所定义的目录的子目录下。当我们删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据。</p>
<p>​	<strong>通俗的来讲，内部表，顾名思义，Hive内部管理的表格，所以hive删除该表格的时候，数据文件也会相应的删除掉</strong></p>
<h3 id="3-2-外部表"><a href="#3-2-外部表" class="headerlink" title="3.2 外部表"></a>3.2 外部表</h3><p>​	外部表，hive并非认为其含有这份数据，删除这个表格的时候并不会删除数据，但是描述表格信息的元数据信息会被删除掉。</p>
<p><strong>实例展示：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create table test(</span><br><span class="line">name string,</span><br><span class="line">friends array&lt;string&gt;,</span><br><span class="line">children map&lt;string, int&gt;,</span><br><span class="line">address struct&lt;street:string, city:string&gt;</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;,&#x27;</span><br><span class="line">collection items terminated by &#x27;_&#x27;</span><br><span class="line">map keys terminated by &#x27;:&#x27;</span><br><span class="line">lines terminated by &#x27;\n&#x27;;</span><br></pre></td></tr></table></figure>

<p>字段解释如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">row format delimited fields terminated by &#x27;,&#x27;     表示列的分隔符是&#x27;,&#x27;</span><br><span class="line">collection items terminated by &#x27;_&#x27;                表示map和array和struct的分隔符</span><br><span class="line">map keys terminated by &#x27;:&#x27;                        表示map中key和value的分隔符</span><br><span class="line">lines terminated by &#x27;\n&#x27;;                         表示行分隔符</span><br></pre></td></tr></table></figure>

<h2 id="4-Hive数据导入"><a href="#4-Hive数据导入" class="headerlink" title="4.Hive数据导入"></a>4.Hive数据导入</h2><h3 id="4-1-向表中装载数据（最常用）"><a href="#4-1-向表中装载数据（最常用）" class="headerlink" title="4.1 向表中装载数据（最常用）"></a>4.1 向表中装载数据（最常用）</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">hive&gt; </span><span class="language-bash">load data [<span class="built_in">local</span>] inpath <span class="string">&#x27;数据的path&#x27;</span> [overwrite] into table student [partition (partcol1=val1,…)];</span></span><br></pre></td></tr></table></figure>

<ol>
<li>local :加上该字段代表从本地加载数据到hive表格，否则从HDFS中加载数据</li>
<li>overwrite：加上该字段代表覆盖表中已有的数据，否则表示最佳</li>
<li>partition：表示上传到指定的分区（分区表的概念）</li>
</ol>
<h3 id="4-2-通过查询语句向表中插入数据"><a href="#4-2-通过查询语句向表中插入数据" class="headerlink" title="4.2 通过查询语句向表中插入数据"></a>4.2 通过查询语句向表中插入数据</h3><ol>
<li><strong>单张表查询结果插入</strong></li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite table student_par </span><br><span class="line">             select id, name from student ; </span><br></pre></td></tr></table></figure>

<p>insert into：以追加数据的方式插入到表或分区，原有数据不会删除</p>
<p>insert overwrite：会覆盖表中已存在的数据</p>
<p>注意：insert不支持插入部分字段</p>
<ol start="2">
<li><strong>多分区插入模式(分区表)</strong></li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; from student</span><br><span class="line">              insert overwrite table student partition(month=&#x27;201707&#x27;)</span><br><span class="line">              select id, name where month=&#x27;201709&#x27;</span><br><span class="line">              insert overwrite table student partition(month=&#x27;201706&#x27;)</span><br><span class="line">              select id, name where month=&#x27;201709&#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="4-3-创建表格的时候通过location指定加载的数据路径"><a href="#4-3-创建表格的时候通过location指定加载的数据路径" class="headerlink" title="4.3 创建表格的时候通过location指定加载的数据路径"></a>4.3 创建表格的时候通过location指定加载的数据路径</h3><p>​	就是在建立表格的之后指定数据路径</p>
<h3 id="4-4-Import数据到指定Hive表格中"><a href="#4-4-Import数据到指定Hive表格中" class="headerlink" title="4.4 Import数据到指定Hive表格中"></a>4.4 Import数据到指定Hive表格中</h3><p>注意：这里需要先使用export把某个表格到处，然后才能把数据导入</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; import table student2  from</span><br><span class="line"> &#x27;/user/hive/warehouse/export/student&#x27;;</span><br></pre></td></tr></table></figure>

<h2 id="5-数据导出"><a href="#5-数据导出" class="headerlink" title="5.数据导出"></a>5.数据导出</h2><ol>
<li><p><strong>Insert导出</strong></p>
<p>导出到本地</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory &#x27;/opt/module/hive/datas/export/student&#x27;</span><br><span class="line">            select * from student;</span><br></pre></td></tr></table></figure>

<p>查询结果格式化后再保存</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">hive(default)&gt;</span><span class="language-bash">insert overwrite <span class="built_in">local</span> directory <span class="string">&#x27;/opt/module/hive/datas/export/student1&#x27;</span></span></span><br><span class="line">           ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;             select * from student;</span><br></pre></td></tr></table></figure>

<p>查询结果保存到HDFS上（其实就是把local去掉就可以）</p>
</li>
<li><p><strong>Hadoop命令导出到本地</strong></p>
<p>其实就是put指令把文件下载下来</p>
</li>
<li><p><strong>Hive Shell命令导出</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive -e &#x27;select * from default.student;&#x27; &gt;</span><br><span class="line"> /opt/module/hive/datas/export/student4.txt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Export导出到HDFS中</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">(defahiveult)&gt; </span><span class="language-bash"><span class="built_in">export</span> table default.student to</span></span><br><span class="line"> &#x27;/user/hive/warehouse/export/student&#x27;;</span><br></pre></td></tr></table></figure>

<p><strong>export和import主要用于两个Hadoop平台集群之间Hive表迁移。</strong></p>
</li>
<li><p>Sqoop导出，后续我会再写一个专门的博客来讲述</p>
</li>
</ol>
<h2 id="6-分区表"><a href="#6-分区表" class="headerlink" title="6.分区表"></a>6.分区表</h2><h3 id="6-1-分区表基本操作"><a href="#6-1-分区表基本操作" class="headerlink" title="6.1 分区表基本操作"></a>6.1 分区表基本操作</h3><p>​	分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。</p>
<p>​	一般来说，分区表大部分都是按照时间来划分，把每一天的数据当作一个分区</p>
<hr>
<p><strong>创建分区表语法：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition(</span><br><span class="line">deptno int, dname string, loc string</span><br><span class="line">)</span><br><span class="line">partitioned by (day string)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p><strong>注意：分区字段不能是表中已经存在的数据，可以将分区字段看作表的伪列。</strong></p>
<hr>
<p><strong>加载数据到分区表中：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/hive/datas/dept_20200401.log&#x27; into table dept_partition partition(day=&#x27;20200401&#x27;);</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/hive/datas/dept_20200402.log&#x27; into table dept_partition partition(day=&#x27;20200402&#x27;);</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/hive/datas/dept_20200403.log&#x27; into table dept_partition partition(day=&#x27;20200403&#x27;);</span><br></pre></td></tr></table></figure>

<h3 id="6-2-二级分区"><a href="#6-2-二级分区" class="headerlink" title="6.2 二级分区"></a>6.2 二级分区</h3><p><strong>创建表格基本语法</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition2(</span><br><span class="line">               deptno int, dname string, loc string</span><br><span class="line">               )</span><br><span class="line">               partitioned by (day string, hour string)</span><br><span class="line">               row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>



<p><strong>加载数据</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module`/hive/datas/dept_20200401.log&#x27; into table</span><br><span class="line">dept_partition2 partition(day=&#x27;20200401&#x27;, hour=&#x27;12&#x27;);</span><br></pre></td></tr></table></figure>

<h3 id="6-3-动态分区"><a href="#6-3-动态分区" class="headerlink" title="6.3 动态分区"></a>6.3 动态分区</h3><p>​	关系型数据库中，对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive的动态分区，需要进行相应的配置。</p>
<p>（1）开启动态分区功能（默认true，开启）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.dynamic.partition=true</span><br></pre></td></tr></table></figure>

<p>（2）设置为非严格模式（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.dynamic.partition.mode=nonstrict</span><br></pre></td></tr></table></figure>

<p>（3）在所有执行MR的节点上，最大一共可以创建多少个动态分区。默认1000</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.max.dynamic.partitions=1000</span><br></pre></td></tr></table></figure>

<p>（4）在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.max.dynamic.partitions.pernode=100</span><br></pre></td></tr></table></figure>

<p>（5）整个MR Job中，最大可以创建多少个HDFS文件。默认100000</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.max.created.files=100000</span><br></pre></td></tr></table></figure>

<p>（6）当有空分区生成时，是否抛出异常。一般不需要设置。默认false</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.error.on.empty.partition=false</span><br></pre></td></tr></table></figure>



<p><strong>创建目标分区表</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition_dy(id int, name string) partitioned by (loc int) row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p><strong>设置动态分区</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert into table dept_partition_dy partition(loc) select deptno, dname, loc from dept;</span><br></pre></td></tr></table></figure>

<h2 id="7-分桶表"><a href="#7-分桶表" class="headerlink" title="7.分桶表"></a>7.分桶表</h2><p>​	分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分。</p>
<p>​	分桶是将数据集分解成更容易管理的若干部分的另一个技术。</p>
<p>​	分区针对的是数据的存储路径；分桶针对的是数据文件。</p>
<p><strong>创建分桶表语法：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create table stu_bucket(id int, name string)</span><br><span class="line">clustered by(id) </span><br><span class="line">into 4 buckets</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p><strong>导入数据到分桶表：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data inpath   &#x27;/student.txt&#x27; into table stu_bucket;</span><br></pre></td></tr></table></figure>

<p><strong>分桶规则：</strong></p>
<p>Hive的分桶采用对分桶字段的值进行哈希，然后除以桶的个数求余的方 式决定该条记录存放在哪个桶当中</p>
<p><strong>注意事项：</strong></p>
<p>（1）reduce的个数设置为-1,让Job自行决定需要用多少个reduce或者将reduce的个数设置为大于等于分桶表的桶数</p>
<p>（2）从hdfs中load数据到分桶表中，避免本地文件找不到问题</p>
<p>（3）不要使用本地模式</p>
<h2 id="8-Hive常用内置函数"><a href="#8-Hive常用内置函数" class="headerlink" title="8. Hive常用内置函数"></a>8. Hive常用内置函数</h2><h3 id="8-1-空字段复制"><a href="#8-1-空字段复制" class="headerlink" title="8.1 空字段复制"></a>8.1 空字段复制</h3><p><strong>函数说明：</strong></p>
<p>​	NVL：给值为NULL的数据赋值，它的格式是NVL( value，default_value)。它的功能是如果value为NULL，则NVL函数返回default_value的值，否则返回value的值，如果两个参数都为NULL ，则返回NULL。</p>
<p><strong>实例：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select comm, nvl(comm,mgr) from emp;</span><br><span class="line">OK</span><br><span class="line">comm    _c1</span><br><span class="line">NULL    7902.0</span><br><span class="line">300.0   300.0</span><br><span class="line">500.0   500.0</span><br><span class="line">NULL    7839.0</span><br><span class="line">1400.0  1400.0</span><br><span class="line">NULL    7839.0</span><br><span class="line">NULL    7839.0</span><br><span class="line">NULL    7566.0</span><br><span class="line">NULL    NULL</span><br><span class="line">0.0     0.0</span><br><span class="line">NULL    7788.0</span><br><span class="line">NULL    7698.0</span><br><span class="line">NULL    7566.0</span><br><span class="line">NULL    7782.0</span><br></pre></td></tr></table></figure>

<h3 id="8-2-CASE-WHEN-ELSE-END"><a href="#8-2-CASE-WHEN-ELSE-END" class="headerlink" title="8.2 CASE WHEN ELSE END"></a>8.2 CASE WHEN ELSE END</h3><p><strong>数据：</strong></p>
<table>
<thead>
<tr>
<th>name</th>
<th>dept_id</th>
<th>sex</th>
</tr>
</thead>
<tbody><tr>
<td>悟空</td>
<td>A</td>
<td>男</td>
</tr>
<tr>
<td>大海</td>
<td>A</td>
<td>男</td>
</tr>
<tr>
<td>宋宋</td>
<td>B</td>
<td>男</td>
</tr>
<tr>
<td>凤姐</td>
<td>A</td>
<td>女</td>
</tr>
<tr>
<td>婷姐</td>
<td>B</td>
<td>女</td>
</tr>
<tr>
<td>婷婷</td>
<td>B</td>
<td>女</td>
</tr>
</tbody></table>
<p><strong>创建表格：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table emp_sex(</span><br><span class="line">name string, </span><br><span class="line">dept_id string, </span><br><span class="line">sex string) </span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;;</span><br><span class="line">load data local inpath &#x27;/opt/module/hive/datas/emp_sex.txt&#x27; into table emp_sex;</span><br></pre></td></tr></table></figure>

<p><strong>查询：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">select </span><br><span class="line">  dept_id,</span><br><span class="line">  sum(case sex when &#x27;男&#x27; then 1 else 0 end) male_count,</span><br><span class="line">  sum(case sex when &#x27;女&#x27; then 1 else 0 end) female_count</span><br><span class="line">from </span><br><span class="line">  emp_sex</span><br><span class="line">group by</span><br><span class="line">  dept_id;</span><br></pre></td></tr></table></figure>

<h3 id="8-3-行转列"><a href="#8-3-行转列" class="headerlink" title="8.3 行转列"></a>8.3 行转列</h3><p><strong>相关函数说明</strong></p>
<p>​	CONCAT(string A&#x2F;col, string B&#x2F;col…)：返回输入字符串连接后的结果，支持任意个输入字符串;</p>
<p>​	CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;</p>
<p>​	注意: CONCAT_WS must be “string or array<string></p>
<p>​	COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。</p>
<p>数据：</p>
<table>
<thead>
<tr>
<th>name</th>
<th>constellation</th>
<th>blood_type</th>
</tr>
</thead>
<tbody><tr>
<td>孙悟空</td>
<td>白羊座</td>
<td>A</td>
</tr>
<tr>
<td>大海</td>
<td>射手座</td>
<td>A</td>
</tr>
<tr>
<td>宋宋</td>
<td>白羊座</td>
<td>B</td>
</tr>
<tr>
<td>猪八戒</td>
<td>白羊座</td>
<td>A</td>
</tr>
<tr>
<td>凤姐</td>
<td>射手座</td>
<td>A</td>
</tr>
<tr>
<td>苍老师</td>
<td>白羊座</td>
<td>B</td>
</tr>
</tbody></table>
<p><strong>需求：</strong></p>
<p>把星座和血型一样的人归类到一起。结果如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">射手座,A            大海|凤姐</span><br><span class="line">白羊座,A            孙悟空|猪八戒</span><br><span class="line">白羊座,B            宋宋|苍老师</span><br></pre></td></tr></table></figure>

<p><strong>创建表格：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table person_info(</span><br><span class="line">name string, </span><br><span class="line">constellation string, </span><br><span class="line">blood_type string) </span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;;</span><br><span class="line">load data local inpath &quot;/opt/module/hive/datas/person_info.txt&quot; into table person_info;</span><br></pre></td></tr></table></figure>

<p><strong>查询语句：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SELECT t1.c_b , CONCAT_WS(&quot;|&quot;,collect_set(t1.name))</span><br><span class="line">FROM (</span><br><span class="line">SELECT NAME ,CONCAT_WS(&#x27;,&#x27;,constellation,blood_type) c_b</span><br><span class="line">FROM person_info</span><br><span class="line">)t1 </span><br><span class="line">GROUP BY t1.c_b</span><br></pre></td></tr></table></figure>

<p>其实行转列或者列转行并不是严格的 非得n行转成1行，要按照函数的应用情况来看</p>
<h3 id="8-4-列转行"><a href="#8-4-列转行" class="headerlink" title="8.4 列转行"></a>8.4 列转行</h3><p><strong>函数说明：</strong></p>
<p>EXPLODE(col)：将hive一列中复杂的array或者map结构拆分成多行。</p>
<p>LATERAL VIEW</p>
<p>用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias</p>
<p>解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</p>
<p><strong>数据准备：</strong></p>
<table>
<thead>
<tr>
<th>movie</th>
<th>category</th>
</tr>
</thead>
<tbody><tr>
<td>《疑犯追踪》</td>
<td>悬疑,动作,科幻,剧情</td>
</tr>
<tr>
<td>《Lie to me》</td>
<td>悬疑,警匪,动作,心理,剧情</td>
</tr>
<tr>
<td>《战狼2》</td>
<td>战争,动作,灾难</td>
</tr>
</tbody></table>
<p><strong>需求：</strong></p>
<p>将电影分类中的数组数据展开。结果如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">《疑犯追踪》      悬疑</span><br><span class="line">《疑犯追踪》      动作</span><br><span class="line">《疑犯追踪》      科幻</span><br><span class="line">《疑犯追踪》      剧情</span><br><span class="line">《Lie to me》   悬疑</span><br><span class="line">《Lie to me》   警匪</span><br><span class="line">《Lie to me》   动作</span><br><span class="line">《Lie to me》   心理</span><br><span class="line">《Lie to me》   剧情</span><br><span class="line">《战狼2》        战争</span><br><span class="line">《战狼2》        动作</span><br><span class="line">《战狼2》        灾难</span><br></pre></td></tr></table></figure>

<p><strong>创建表格：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create table movie_info(</span><br><span class="line">    movie string, </span><br><span class="line">    category string) </span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;;</span><br><span class="line">load data local inpath &quot;/opt/module/hive/datas/movie_info.txt&quot; into table movie_info;</span><br></pre></td></tr></table></figure>

<p><strong>查询语句：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SELECT movie,category_name </span><br><span class="line">FROM movie_info </span><br><span class="line">lateral VIEW</span><br><span class="line">explode(split(category,&quot;,&quot;)) movie_info_tmp  AS category_name ;</span><br></pre></td></tr></table></figure>

<p>解读：上述经过split函数之后，就成了一个字符串数组，然后我们就可以使用explode函数把这个数组变为多行数据。</p>
<h3 id="8-5-窗口函数"><a href="#8-5-窗口函数" class="headerlink" title="8.5 窗口函数"></a>8.5 窗口函数</h3><p><strong>相关函数说明</strong></p>
<p>OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的改变而变化。</p>
<p>CURRENT ROW：当前行</p>
<p>n PRECEDING：往前n行数据</p>
<p>n FOLLOWING：往后n行数据</p>
<p>UNBOUNDED：起点，</p>
<p>​         UNBOUNDED PRECEDING 表示从前面的起点， </p>
<p>​    UNBOUNDED FOLLOWING表示到后面的终点</p>
<p>LAG(col,n,default_val)：往前第n行数据</p>
<p>LEAD(col,n, default_val)：往后第n行数据</p>
<p>NTILE(n)：把有序窗口的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型。</p>
<p><strong>数据准备：name,orderdate,cost</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">jack,2017-01-01,10</span><br><span class="line">tony,2017-01-02,15</span><br><span class="line">jack,2017-02-03,23</span><br><span class="line">tony,2017-01-04,29</span><br><span class="line">jack,2017-01-05,46</span><br><span class="line">jack,2017-04-06,42</span><br><span class="line">tony,2017-01-07,50</span><br><span class="line">jack,2017-01-08,55</span><br><span class="line">mart,2017-04-08,62</span><br><span class="line">mart,2017-04-09,68</span><br><span class="line">neil,2017-05-10,12</span><br><span class="line">mart,2017-04-11,75</span><br><span class="line">neil,2017-06-12,80</span><br><span class="line">mart,2017-04-13,94</span><br></pre></td></tr></table></figure>

<p><strong>需求：</strong></p>
<p>（1）查询在2017年4月份购买过的顾客及总人数</p>
<p>（2）查询顾客的购买明细及月购买总额</p>
<p>（3）上述的场景, 将每个顾客的cost按照日期进行累加</p>
<p>（4）查询每个顾客上次的购买时间</p>
<p>（5）查询前20%时间的订单信息</p>
<p><strong>创建表格：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table business(</span><br><span class="line">name string, </span><br><span class="line">orderdate string,</span><br><span class="line">cost int</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;,&#x27;;</span><br><span class="line">load data local inpath &quot;/opt/module/hive/datas/business.txt&quot; into table business;</span><br></pre></td></tr></table></figure>

<p><strong>查询命令：</strong></p>
<ol>
<li><p>查询在2017年4月份购买过的顾客及总人数</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select name,count(*) over () </span><br><span class="line">from business </span><br><span class="line">where substring(orderdate,1,7) = &#x27;2017-04&#x27; </span><br><span class="line">group by name;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>查询顾客的购买明细及月购买总额</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select name,orderdate,cost,sum(cost) over(partition by month(orderdate)) from</span><br><span class="line">business;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>将每个顾客的cost按照日期进行累加</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">select name,orderdate,cost, </span><br><span class="line">sum(cost) over() as sample1,--所有行相加 </span><br><span class="line">sum(cost) over(partition by name) as sample2,--按name分组，组内数据相加 </span><br><span class="line">sum(cost) over(partition by name order by orderdate) as sample3,--按name分组，组内数据累加 </span><br><span class="line">sum(cost) over(partition by name order by orderdate rows between UNBOUNDED PRECEDING and current row ) as sample4 ,--和sample3一样,由起点到当前行的聚合 </span><br><span class="line">sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING and current row) as sample5, --当前行和前面一行做聚合 </span><br><span class="line">sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING AND 1 FOLLOWING ) as sample6,--当前行和前边一行及后面一行 </span><br><span class="line">sum(cost) over(partition by name order by orderdate rows between current row and UNBOUNDED FOLLOWING ) as sample7 --当前行及后面所有行 </span><br><span class="line">from business;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>查看顾客上次的购买时间</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select name,orderdate,cost, </span><br><span class="line">lag(orderdate,1,&#x27;1900-01-01&#x27;) over(partition by name order by orderdate ) as time1, lag(orderdate,2) over (partition by name order by orderdate) as time2 </span><br><span class="line">from business;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>查询前20%时间的订单信息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select * from (</span><br><span class="line">    select name,orderdate,cost, ntile(5) over(order by orderdate) sorted</span><br><span class="line">    from business</span><br><span class="line">) t</span><br><span class="line">where sorted = 1;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<p>over(): 会为每条数据都开启一个窗口. 默认的窗口大小就是当前数据集的大小.<br>over(partition by ….) : 会按照指定的字段进行分区， 将分区字段的值相同的数据划分到相同的区。<br>                          每个区中的每条数据都会开启一个窗口.每条数据的窗口大小默认为当前分区数据集的大小.<br>over(order by ….) : 会在窗口中按照指定的字段对数据进行排序.<br>                      会为每条数据都开启一个窗口,默认的窗口大小为从数据集开头（起始行）开始到当前行.  </p>
<p>over(partition by …. order by ….) :会按照指定的字段进行分区， 将分区字段的值相同的数据划分到相同的区,<br>                                       在每个区中会按照指定的字段进行排序.<br>                                       会为每条数据都开启一个窗口,默认的窗口大小为当前分区中从数据集开始到当前行.                                            over(partition by … order by … rows between … and ….) : 指定每条数据的窗口大小.</p>
<h3 id="8-6-Rank"><a href="#8-6-Rank" class="headerlink" title="8.6 Rank"></a>8.6 Rank</h3><p><strong>函数说明：</strong></p>
<p>RANK() 排序相同时会重复，总数不会变</p>
<p>DENSE_RANK() 排序相同时会重复，总数会减少</p>
<p>ROW_NUMBER() 会根据顺序计算</p>
<p><strong>数据准备：</strong></p>
<table>
<thead>
<tr>
<th>name</th>
<th>subject</th>
<th>score</th>
</tr>
</thead>
<tbody><tr>
<td>孙悟空</td>
<td>语文</td>
<td>87</td>
</tr>
<tr>
<td>孙悟空</td>
<td>数学</td>
<td>95</td>
</tr>
<tr>
<td>孙悟空</td>
<td>英语</td>
<td>68</td>
</tr>
<tr>
<td>大海</td>
<td>语文</td>
<td>94</td>
</tr>
<tr>
<td>大海</td>
<td>数学</td>
<td>56</td>
</tr>
<tr>
<td>大海</td>
<td>英语</td>
<td>84</td>
</tr>
<tr>
<td>宋宋</td>
<td>语文</td>
<td>64</td>
</tr>
<tr>
<td>宋宋</td>
<td>数学</td>
<td>86</td>
</tr>
<tr>
<td>宋宋</td>
<td>英语</td>
<td>84</td>
</tr>
<tr>
<td>婷婷</td>
<td>语文</td>
<td>65</td>
</tr>
<tr>
<td>婷婷</td>
<td>数学</td>
<td>85</td>
</tr>
<tr>
<td>婷婷</td>
<td>英语</td>
<td>78</td>
</tr>
</tbody></table>
<p><strong>需求：</strong></p>
<p>计算每门学科成绩排名</p>
<p><strong>创建表格：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create table score(</span><br><span class="line">name string,</span><br><span class="line">subject string, </span><br><span class="line">score int) </span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;;</span><br><span class="line">load data local inpath &#x27;/opt/module/hive/datas/score.txt&#x27; into table score;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><strong>查询语句：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">select name,</span><br><span class="line">subject,</span><br><span class="line">score,</span><br><span class="line">rank() over(partition by subject order by score desc) rp,</span><br><span class="line">dense_rank() over(partition by subject order by score desc) drp,</span><br><span class="line">row_number() over(partition by subject order by score desc) rmp</span><br><span class="line">from score;</span><br><span class="line"></span><br><span class="line">name    subject score   rp      drp     rmp</span><br><span class="line">孙悟空  数学    95      1       1       1</span><br><span class="line">宋宋    数学    86      2       2       2</span><br><span class="line">婷婷    数学    85      3       3       3</span><br><span class="line">大海    数学    56      4       4       4</span><br><span class="line">宋宋    英语    84      1       1       1</span><br><span class="line">大海    英语    84      1       1       2</span><br><span class="line">婷婷    英语    78      3       2       3</span><br><span class="line">孙悟空  英语    68      4       3       4</span><br><span class="line">大海    语文    94      1       1       1</span><br><span class="line">孙悟空  语文    87      2       2       2</span><br><span class="line">婷婷    语文    65      3       3       3</span><br><span class="line">宋宋    语文    64      4       4       4</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="8-7-其它常用函数"><a href="#8-7-其它常用函数" class="headerlink" title="8.7 其它常用函数"></a>8.7 其它常用函数</h3><h4 id="8-7-1-常用日期函数"><a href="#8-7-1-常用日期函数" class="headerlink" title="8.7.1 常用日期函数"></a>8.7.1 常用日期函数</h4><ol>
<li>unix_timestamp:返回当前或指定时间的时间戳	<br>select unix_timestamp();<br>select unix_timestamp(“2020-10-28”,’yyyy-MM-dd’);</li>
<li>from_unixtime：将时间戳转为日期格式<br>select from_unixtime(1603843200);</li>
<li>current_date：当前日期<br>select current_date;</li>
<li>current_timestamp：当前的日期加时间<br>select current_timestamp;</li>
<li>to_date：抽取日期部分<br>select to_date(‘2020-10-28 12:12:12’);</li>
<li>year：获取年<br>select year(‘2020-10-28 12:12:12’);</li>
<li>month：获取月<br>select month(‘2020-10-28 12:12:12’);</li>
<li>day：获取日<br>select day(‘2020-10-28 12:12:12’);</li>
<li>hour：获取时<br>select hour(‘2020-10-28 12:13:14’);</li>
<li>minute：获取分<br>select minute(‘2020-10-28 12:13:14’);</li>
<li>second：获取秒<br>select second(‘2020-10-28 12:13:14’);</li>
<li>weekofyear：当前时间是一年中的第几周<br>select weekofyear(‘2020-10-28 12:12:12’);</li>
<li>dayofmonth：当前时间是一个月中的第几天<br>select dayofmonth(‘2020-10-28 12:12:12’);</li>
<li>months_between： 两个日期间的月份<br>select months_between(‘2020-04-01’,’2020-10-28’);</li>
<li>add_months：日期加减月<br>select add_months(‘2020-10-28’,-3);</li>
<li>datediff：两个日期相差的天数<br>select datediff(‘2020-11-04’,’2020-10-28’);</li>
<li>date_add：日期加天数<br>select date_add(‘2020-10-28’,4);</li>
<li>date_sub：日期减天数<br>select date_sub(‘2020-10-28’,-4);</li>
<li>last_day：日期的当月的最后一天<br>select last_day(‘2020-02-30’);</li>
<li>date_format(): 格式化日期<br>select date_format(‘2020-10-28 12:12:12’,’yyyy&#x2F;MM&#x2F;dd HH:mm:ss’);</li>
</ol>
<h4 id="8-7-2-常用取整函数"><a href="#8-7-2-常用取整函数" class="headerlink" title="8.7.2 常用取整函数"></a>8.7.2 常用取整函数</h4><ol start="21">
<li>round： 四舍五入<br>select round(3.14);<br>select round(3.54);</li>
<li>ceil：  向上取整<br>select ceil(3.14);<br>select ceil(3.54);</li>
</ol>
<p>floor： 向下取整</p>
<ol start="23">
<li>select floor(3.14);<br>select floor(3.54);</li>
</ol>
<h4 id="8-7-3-常用字符串操作函数"><a href="#8-7-3-常用字符串操作函数" class="headerlink" title="8.7.3 常用字符串操作函数"></a>8.7.3 常用字符串操作函数</h4><ol start="24">
<li>upper： 转大写<br>select upper(‘low’);</li>
<li>lower： 转小写<br>select lower(‘low’);</li>
<li>length： 长度<br>select length(“atguigu”);</li>
<li>trim：  前后去空格<br>select trim(“ atguigu “);</li>
<li>lpad： 向左补齐，到指定长度<br>select lpad(‘atguigu’,9,’g’);</li>
<li>rpad：  向右补齐，到指定长度<br>select rpad(‘atguigu’,9,’g’);</li>
<li>regexp_replace：使用正则表达式匹配目标字符串，匹配成功后替换！<br>SELECT regexp_replace(‘2020&#x2F;10&#x2F;25’, ‘&#x2F;‘, ‘-‘);</li>
</ol>
<h4 id="8-7-4-集合操作"><a href="#8-7-4-集合操作" class="headerlink" title="8.7.4 集合操作"></a>8.7.4 集合操作</h4><ol start="31">
<li>size： 集合中元素的个数<br>select size(friends) from test3;</li>
<li>map_keys： 返回map中的key<br>select map_keys(children) from test3;</li>
<li>map_values: 返回map中的value<br>select map_values(children) from test3;</li>
<li>array_contains: 判断array中是否包含某个元素<br>select array_contains(friends,’bingbing’) from test3;</li>
<li>sort_array： 将array中的元素排序<br>select sort_array(friends) from test3;</li>
</ol>
<h3 id="8-8-自定义UDF函数"><a href="#8-8-自定义UDF函数" class="headerlink" title="8.8 自定义UDF函数"></a>8.8 自定义UDF函数</h3><p><strong>需求：</strong></p>
<p>自定义一个UDF实现计算给定字符串的长度，例如：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">hive(default)&gt; </span><span class="language-bash">select my_len(<span class="string">&quot;abcd&quot;</span>);</span></span><br><span class="line">4</span><br></pre></td></tr></table></figure>

<p><strong>实现类：</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.hive;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDFArgumentException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.udf.generic.GenericUDF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义UDF函数，需要继承GenericUDF类</span></span><br><span class="line"><span class="comment"> * 需求: 计算指定字符串的长度</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyStringLength</span> <span class="keyword">extends</span> <span class="title class_">GenericUDF</span> &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> arguments 输入参数类型的鉴别器对象</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 返回值类型的鉴别器对象</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> UDFArgumentException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> ObjectInspector <span class="title function_">initialize</span><span class="params">(ObjectInspector[] arguments)</span> <span class="keyword">throws</span> UDFArgumentException &#123;</span><br><span class="line">        <span class="comment">// 判断输入参数的个数</span></span><br><span class="line">        <span class="keyword">if</span>(arguments.length !=<span class="number">1</span>)&#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">UDFArgumentLengthException</span>(<span class="string">&quot;Input Args Length Error!!!&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 判断输入参数的类型</span></span><br><span class="line">        <span class="keyword">if</span>(!arguments[<span class="number">0</span>].getCategory().equals(ObjectInspector.Category.PRIMITIVE))&#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">UDFArgumentTypeException</span>(<span class="number">0</span>,<span class="string">&quot;Input Args Type Error!!!&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//函数本身返回值为int，需要返回int类型的鉴别器对象</span></span><br><span class="line">        <span class="keyword">return</span> PrimitiveObjectInspectorFactory.javaIntObjectInspector;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 函数的逻辑处理</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> arguments 输入的参数</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 返回值</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> HiveException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Object <span class="title function_">evaluate</span><span class="params">(DeferredObject[] arguments)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">       <span class="keyword">if</span>(arguments[<span class="number">0</span>].get() == <span class="literal">null</span>)&#123;</span><br><span class="line">           <span class="keyword">return</span> <span class="number">0</span> ;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> arguments[<span class="number">0</span>].get().toString().length();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getDisplayString</span><span class="params">(String[] children)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>打成jar包上传到服务器&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;myudf.jar</strong></p>
<p><strong>将jar包添加到hive的classpath</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; add jar /opt/module/hive/datas/myudf.jar;</span><br></pre></td></tr></table></figure>

<p><strong>创建临时函数与开发好的javaclass关联</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create temporary function my_len as &quot;com.atguigu.hive.MyStringLength&quot;;</span><br></pre></td></tr></table></figure>

<p><strong>即可在hql中使用自定义的函数</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select ename,my_len(ename) ename_len from emp;</span><br></pre></td></tr></table></figure>

<h3 id="8-9-自定义UDTF函数"><a href="#8-9-自定义UDTF函数" class="headerlink" title="8.9 自定义UDTF函数"></a>8.9 自定义UDTF函数</h3><p><strong>需求：</strong></p>
<p>自定义一个UDTF实现将一个任意分割符的字符串切割成独立的单词，例如：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">hive(default)&gt; </span><span class="language-bash">select myudtf(<span class="string">&quot;hello,world,hadoop,hive&quot;</span>, <span class="string">&quot;,&quot;</span>);</span></span><br><span class="line"></span><br><span class="line">hello</span><br><span class="line">world</span><br><span class="line">hadoop</span><br><span class="line">hive</span><br></pre></td></tr></table></figure>

<p><strong>代码实现：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.udtf;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDFArgumentException;</span><br><span class="line">import org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line">import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</span><br><span class="line"></span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">public class MyUDTF extends GenericUDTF &#123;</span><br><span class="line"></span><br><span class="line">    private ArrayList&lt;String&gt; outList = new ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public StructObjectInspector initialize(StructObjectInspector argOIs) throws UDFArgumentException &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        //1.定义输出数据的列名和类型</span><br><span class="line">        List&lt;String&gt; fieldNames = new ArrayList&lt;&gt;();</span><br><span class="line">        List&lt;ObjectInspector&gt; fieldOIs = new ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        //2.添加输出数据的列名和类型</span><br><span class="line">        fieldNames.add(&quot;lineToWord&quot;);</span><br><span class="line">        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class="line"></span><br><span class="line">        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void process(Object[] args) throws HiveException &#123;</span><br><span class="line">        </span><br><span class="line">        //1.获取原始数据</span><br><span class="line">        String arg = args[0].toString();</span><br><span class="line"></span><br><span class="line">        //2.获取数据传入的第二个参数，此处为分隔符</span><br><span class="line">        String splitKey = args[1].toString();</span><br><span class="line"></span><br><span class="line">        //3.将原始数据按照传入的分隔符进行切分</span><br><span class="line">        String[] fields = arg.split(splitKey);</span><br><span class="line"></span><br><span class="line">        //4.遍历切分后的结果，并写出</span><br><span class="line">        for (String field : fields) &#123;</span><br><span class="line"></span><br><span class="line">            //集合为复用的，首先清空集合</span><br><span class="line">            outList.clear();</span><br><span class="line"></span><br><span class="line">            //将每一个单词添加至集合</span><br><span class="line">            outList.add(field);</span><br><span class="line"></span><br><span class="line">            //将集合内容写出</span><br><span class="line">            forward(outList);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void close() throws HiveException &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>打成jar包上传到服务器&#x2F;opt&#x2F;module&#x2F;hive&#x2F;data&#x2F;myudtf.jar</strong></p>
<p><strong>将jar包添加到hive的classpath下</strong></p>
<p><strong>创建临时函数与开发好的javaclass关联</strong></p>
<p><strong>使用自定义的函数</strong></p>
<h2 id="9-文件存储格式"><a href="#9-文件存储格式" class="headerlink" title="9.文件存储格式"></a>9.文件存储格式</h2><p>​	Hive支持的存储数据的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。</p>
<h3 id="9-1列式存储和行式存储"><a href="#9-1列式存储和行式存储" class="headerlink" title="9.1列式存储和行式存储"></a>9.1列式存储和行式存储</h3><p><img src="/2022/08/30/Hive/hive%E6%A6%82%E8%BF%B0/1661938785375.png" alt="1661938785375"></p>
<p>​	如图所示左边为逻辑表，右边第一个为行式存储，第二个为列式存储。</p>
<ol>
<li><p><strong>行存储的特点</strong></p>
<p>​	查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</p>
</li>
<li><p><strong>列存储的特点</strong></p>
<p>​	因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。</p>
</li>
</ol>
<p>​	<strong>TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；</strong></p>
<p>​	<strong>ORC和PARQUET是基于列式存储的。</strong></p>
<h3 id="9-2-textFile格式"><a href="#9-2-textFile格式" class="headerlink" title="9.2 textFile格式"></a>9.2 textFile格式</h3><p>​	默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。</p>
<h3 id="9-3-Orc格式"><a href="#9-3-Orc格式" class="headerlink" title="9.3 Orc格式"></a>9.3 Orc格式</h3><p>​	Orc (Optimized Row Columnar)是Hive 0.11版里引入的新的存储格式。</p>
<p>​	如下图所示可以看到每个Orc文件由1个或多个stripe组成，每个stripe一般为HDFS的块大小，每一个stripe包含多条记录，这些记录按照列进行独立存储，对应到Parquet中的row group的概念。每个Stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer：</p>
<p><img src="/2022/08/30/Hive/hive%E6%A6%82%E8%BF%B0/1661939117144.png" alt="1661939117144"></p>
<p>​	1）Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引应该只是记录某行的各字段在Row Data中的offset。</p>
<p>​	2）Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。</p>
<p>​	3）Stripe Footer：存的是各个Stream的类型，长度等信息。</p>
<p>​	每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。</p>
<h3 id="9-4-Parquet格式"><a href="#9-4-Parquet格式" class="headerlink" title="9.4 Parquet格式"></a>9.4 Parquet格式</h3><p>​	Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。</p>
<p>（1）行组(Row Group)：每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，类似于orc的stripe的概念。</p>
<p>（2）列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。</p>
<p>（3）页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。</p>
<p>​	通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式。</p>
<p><img src="/2022/08/30/Hive/hive%E6%A6%82%E8%BF%B0/1661939577441.png" alt="1661939577441"></p>
<p>​	上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic<br>Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。</p>
<h3 id="9-5存储方式和压缩总结"><a href="#9-5存储方式和压缩总结" class="headerlink" title="9.5存储方式和压缩总结"></a>9.5存储方式和压缩总结</h3><p>​	在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Xiang Liu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2022/08/30/Hive/hive%E6%A6%82%E8%BF%B0/">http://example.com/2022/08/30/Hive/hive%E6%A6%82%E8%BF%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">The Xiang Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/UDF/">UDF</a><a class="post-meta__tags" href="/tags/UDTF/">UDTF</a><a class="post-meta__tags" href="/tags/%E5%88%86%E5%8C%BA%E8%A1%A8/">分区表</a><a class="post-meta__tags" href="/tags/%E5%88%86%E6%A1%B6%E8%A1%A8/">分桶表</a><a class="post-meta__tags" href="/tags/%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0/">开窗函数</a><a class="post-meta__tags" href="/tags/Hive/">Hive</a></div><div class="post_share"><div class="social-share" data-image="/../image/bg.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/08/31/Flume/flume%E6%A6%82%E8%BF%B0/"><img class="prev-cover" src="/../image/bg.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">flume概述</div></div></a></div><div class="next-post pull-right"><a href="/2022/08/28/Zookeper/%E6%9C%AA%E5%91%BD%E5%90%8D/"><img class="next-cover" src="/../image/bg.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Zookeeper</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/./image/title.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Xiang Liu</div><div class="author-info__description">欢迎访问我的博客</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">28</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">191</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/theXiangCode" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://mail.qq.com/" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">这几天心理颇不宁静</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Hive%E7%9A%84%E5%87%A0%E7%A7%8D%E5%90%AF%E5%8A%A8%E6%96%B9%E5%BC%8F"><span class="toc-text">1. Hive的几种启动方式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%A8%A1%E5%BC%8F"><span class="toc-text">1.1 命令行模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E4%BD%BF%E7%94%A8%E5%85%83%E6%95%B0%E6%8D%AE%E6%9C%8D%E5%8A%A1%E7%9A%84%E6%96%B9%E5%BC%8F%E8%AE%BF%E9%97%AEHive"><span class="toc-text">1.2 使用元数据服务的方式访问Hive</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E4%BD%BF%E7%94%A8JDBC%E6%96%B9%E5%BC%8F%E6%9D%A5%E8%AE%BF%E9%97%AEHive"><span class="toc-text">1.3 使用JDBC方式来访问Hive</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-hiveserver2%E5%92%8Cmetastore%E7%9A%84%E8%81%94%E7%B3%BB"><span class="toc-text">1.4 hiveserver2和metastore的联系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-%E6%80%BB%E7%BB%93"><span class="toc-text">1.5 总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Hive%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-text">2.Hive的数据类型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1Hive%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-text">2.1Hive的基本数据类型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E9%9B%86%E5%90%88%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-text">2.2 集合数据类型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Hive%E5%88%9B%E5%BB%BA%E8%A1%A8%E6%A0%BC"><span class="toc-text">3.Hive创建表格</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E5%86%85%E9%83%A8%E8%A1%A8"><span class="toc-text">3.1 内部表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E5%A4%96%E9%83%A8%E8%A1%A8"><span class="toc-text">3.2 外部表</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Hive%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5"><span class="toc-text">4.Hive数据导入</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%90%91%E8%A1%A8%E4%B8%AD%E8%A3%85%E8%BD%BD%E6%95%B0%E6%8D%AE%EF%BC%88%E6%9C%80%E5%B8%B8%E7%94%A8%EF%BC%89"><span class="toc-text">4.1 向表中装载数据（最常用）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E9%80%9A%E8%BF%87%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E5%90%91%E8%A1%A8%E4%B8%AD%E6%8F%92%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-text">4.2 通过查询语句向表中插入数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E5%88%9B%E5%BB%BA%E8%A1%A8%E6%A0%BC%E7%9A%84%E6%97%B6%E5%80%99%E9%80%9A%E8%BF%87location%E6%8C%87%E5%AE%9A%E5%8A%A0%E8%BD%BD%E7%9A%84%E6%95%B0%E6%8D%AE%E8%B7%AF%E5%BE%84"><span class="toc-text">4.3 创建表格的时候通过location指定加载的数据路径</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-Import%E6%95%B0%E6%8D%AE%E5%88%B0%E6%8C%87%E5%AE%9AHive%E8%A1%A8%E6%A0%BC%E4%B8%AD"><span class="toc-text">4.4 Import数据到指定Hive表格中</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA"><span class="toc-text">5.数据导出</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%88%86%E5%8C%BA%E8%A1%A8"><span class="toc-text">6.分区表</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E5%88%86%E5%8C%BA%E8%A1%A8%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-text">6.1 分区表基本操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E4%BA%8C%E7%BA%A7%E5%88%86%E5%8C%BA"><span class="toc-text">6.2 二级分区</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA"><span class="toc-text">6.3 动态分区</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E5%88%86%E6%A1%B6%E8%A1%A8"><span class="toc-text">7.分桶表</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-Hive%E5%B8%B8%E7%94%A8%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0"><span class="toc-text">8. Hive常用内置函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-%E7%A9%BA%E5%AD%97%E6%AE%B5%E5%A4%8D%E5%88%B6"><span class="toc-text">8.1 空字段复制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-CASE-WHEN-ELSE-END"><span class="toc-text">8.2 CASE WHEN ELSE END</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-%E8%A1%8C%E8%BD%AC%E5%88%97"><span class="toc-text">8.3 行转列</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-4-%E5%88%97%E8%BD%AC%E8%A1%8C"><span class="toc-text">8.4 列转行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-5-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0"><span class="toc-text">8.5 窗口函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-6-Rank"><span class="toc-text">8.6 Rank</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-7-%E5%85%B6%E5%AE%83%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0"><span class="toc-text">8.7 其它常用函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#8-7-1-%E5%B8%B8%E7%94%A8%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0"><span class="toc-text">8.7.1 常用日期函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-7-2-%E5%B8%B8%E7%94%A8%E5%8F%96%E6%95%B4%E5%87%BD%E6%95%B0"><span class="toc-text">8.7.2 常用取整函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-7-3-%E5%B8%B8%E7%94%A8%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%93%8D%E4%BD%9C%E5%87%BD%E6%95%B0"><span class="toc-text">8.7.3 常用字符串操作函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-7-4-%E9%9B%86%E5%90%88%E6%93%8D%E4%BD%9C"><span class="toc-text">8.7.4 集合操作</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-8-%E8%87%AA%E5%AE%9A%E4%B9%89UDF%E5%87%BD%E6%95%B0"><span class="toc-text">8.8 自定义UDF函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-9-%E8%87%AA%E5%AE%9A%E4%B9%89UDTF%E5%87%BD%E6%95%B0"><span class="toc-text">8.9 自定义UDTF函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F"><span class="toc-text">9.文件存储格式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8%E5%92%8C%E8%A1%8C%E5%BC%8F%E5%AD%98%E5%82%A8"><span class="toc-text">9.1列式存储和行式存储</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-textFile%E6%A0%BC%E5%BC%8F"><span class="toc-text">9.2 textFile格式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3-Orc%E6%A0%BC%E5%BC%8F"><span class="toc-text">9.3 Orc格式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-4-Parquet%E6%A0%BC%E5%BC%8F"><span class="toc-text">9.4 Parquet格式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-5%E5%AD%98%E5%82%A8%E6%96%B9%E5%BC%8F%E5%92%8C%E5%8E%8B%E7%BC%A9%E6%80%BB%E7%BB%93"><span class="toc-text">9.5存储方式和压缩总结</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/09/10/Java/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86/" title="Java基本知识点介绍"><img src="/../image/bg.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Java基本知识点介绍"/></a><div class="content"><a class="title" href="/2022/09/10/Java/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86/" title="Java基本知识点介绍">Java基本知识点介绍</a><time datetime="2022-09-10T12:44:35.832Z" title="发表于 2022-09-10 20:44:35">2022-09-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86/" title="计算机网络知识点介绍"><img src="/../image/bg.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机网络知识点介绍"/></a><div class="content"><a class="title" href="/2022/09/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86/" title="计算机网络知识点介绍">计算机网络知识点介绍</a><time datetime="2022-09-10T12:26:42.061Z" title="发表于 2022-09-10 20:26:42">2022-09-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/10/Spark/Spark%E6%BA%90%E7%A0%81%E5%86%85%E6%A0%B8/" title="Spark内核"><img src="/../image/bg.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Spark内核"/></a><div class="content"><a class="title" href="/2022/09/10/Spark/Spark%E6%BA%90%E7%A0%81%E5%86%85%E6%A0%B8/" title="Spark内核">Spark内核</a><time datetime="2022-09-10T02:54:42.307Z" title="发表于 2022-09-10 10:54:42">2022-09-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/08/Spark/Spark%E5%B0%8F%E6%A1%88%E4%BE%8B/" title="Spark一个小案例"><img src="/../image/bg.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Spark一个小案例"/></a><div class="content"><a class="title" href="/2022/09/08/Spark/Spark%E5%B0%8F%E6%A1%88%E4%BE%8B/" title="Spark一个小案例">Spark一个小案例</a><time datetime="2022-09-08T15:24:18.328Z" title="发表于 2022-09-08 23:24:18">2022-09-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/" title="Spark核心编程"><img src="/../image/bg.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Spark核心编程"/></a><div class="content"><a class="title" href="/2022/09/06/Spark/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/" title="Spark核心编程">Spark核心编程</a><time datetime="2022-09-06T11:27:39.924Z" title="发表于 2022-09-06 19:27:39">2022-09-06</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/../image/bg.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2022 <i style="color:#FF6A6A;animation: announ_animation 0.8s linear infinite;" class="fa fa-heartbeat"></i> Xiang Liu</div><div class="footer_custom_text">兽人永不为奴!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><script src="https://gcore.jsdelivr.net/gh/xiabo2/CDN@latest/fishes.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>